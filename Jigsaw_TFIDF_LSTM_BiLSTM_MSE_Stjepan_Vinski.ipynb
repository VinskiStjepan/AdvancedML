{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c61a8b5",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc97645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U pandas numpy scikit-learn scipy matplotlib tqdm nltk\n",
    "#%pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f9bcf",
   "metadata": {},
   "source": [
    "## Imports + config + device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eae9b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1+cu121\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "import os, re, random, time\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff2c41",
   "metadata": {},
   "source": [
    "## Reproducibility + global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0cfca64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(seed=42, data_path='./data/jigsaw/train.csv', subsample_n=200000, test_size=0.1, val_size=0.1, label_threshold=0.5, max_features=50000, ngram_range=(1, 2), min_df=2, batch_size=256, epochs=3, lr=0.001, weight_decay=0.0001, optimizer='adam', momentum=0.9, grad_clip_norm=1.0, seq_max_len=200, vocab_size=50000, emb_dim=128, lstm_hidden=128, lstm_layers=1, dropout=0.3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    data_path: str = \"./data/jigsaw/train.csv\"\n",
    "    subsample_n: int | None = 200_000  # set None to use full file (big)\n",
    "    test_size: float = 0.1\n",
    "    val_size: float = 0.1  # fraction of remaining train for validation\n",
    "    label_threshold: float = 0.5\n",
    "\n",
    "    # TF-IDF\n",
    "    max_features: int = 50_000\n",
    "    ngram_range: tuple = (1, 2)\n",
    "    min_df: int = 2\n",
    "\n",
    "    # PyTorch training\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 3\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    optimizer: str = \"adam\"  # \"adam\" or \"sgd\"\n",
    "    momentum: float = 0.9\n",
    "    grad_clip_norm: float | None = 1.0\n",
    "\n",
    "    # Sequence model\n",
    "    seq_max_len: int = 200\n",
    "    vocab_size: int = 50_000\n",
    "    emb_dim: int = 128\n",
    "    lstm_hidden: int = 128\n",
    "    lstm_layers: int = 1\n",
    "    dropout: float = 0.3\n",
    "\n",
    "cfg = CFG()\n",
    "seed_everything(cfg.seed)\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3697cc",
   "metadata": {},
   "source": [
    "## Load Jigsaw train.csv + label + subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70fc5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.103228</td>\n",
       "      <td>0.079970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.197294</td>\n",
       "      <td>0.271247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          label\n",
       "count  200000.000000  200000.000000\n",
       "mean        0.103228       0.079970\n",
       "std         0.197294       0.271247\n",
       "min         0.000000       0.000000\n",
       "25%         0.000000       0.000000\n",
       "50%         0.000000       0.000000\n",
       "75%         0.166667       0.000000\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(cfg.data_path)\n",
    "print(df.shape)\n",
    "df = df.dropna(subset=[\"comment_text\", \"target\"]).reset_index(drop=True)\n",
    "\n",
    "df[\"label\"] = (df[\"target\"] >= cfg.label_threshold).astype(int)\n",
    "\n",
    "# Optional: subsample (keep label distribution)\n",
    "if cfg.subsample_n is not None and len(df) > cfg.subsample_n:\n",
    "    df, _ = train_test_split(\n",
    "        df,\n",
    "        train_size=cfg.subsample_n,\n",
    "        stratify=df[\"label\"],\n",
    "        random_state=cfg.seed\n",
    "    )\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "df[[\"target\", \"label\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477226a",
   "metadata": {},
   "source": [
    "## Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81690083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 162000 Val: 18000 Test: 20000\n",
      "Pos rate train/val/test: 0.07997530864197532 0.07994444444444444 0.07995\n"
     ]
    }
   ],
   "source": [
    "X = df[\"comment_text\"].astype(str).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# First: split out test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=cfg.test_size,\n",
    "    stratify=y,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "# Then: split train/val from remaining\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval,\n",
    "    test_size=cfg.val_size,\n",
    "    stratify=y_trainval,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n",
    "print(\"Pos rate train/val/test:\", y_train.mean(), y_val.mean(), y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d6fe1",
   "metadata": {},
   "source": [
    "## Preprocessing: tokenization + stemming + lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b0abd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK downloads (run once)\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "_token_re = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "def basic_tokenize(text: str):\n",
    "    return _token_re.findall(text.lower())\n",
    "\n",
    "def stem_text(text: str) -> str:\n",
    "    toks = basic_tokenize(text)\n",
    "    toks = [stemmer.stem(t) for t in toks]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def lemma_text(text: str) -> str:\n",
    "    toks = basic_tokenize(text)\n",
    "    toks = [lemmatizer.lemmatize(t) for t in toks]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Choose one:\n",
    "PREPROC_MODE = \"raw\"  # \"raw\" | \"stem\" | \"lemma\"\n",
    "\n",
    "def preprocess_array(arr):\n",
    "    if PREPROC_MODE == \"raw\":\n",
    "        return arr\n",
    "    fn = stem_text if PREPROC_MODE == \"stem\" else lemma_text\n",
    "    return np.array([fn(x) for x in tqdm(arr, desc=f\"preprocess={PREPROC_MODE}\")], dtype=object)\n",
    "\n",
    "X_train_p = preprocess_array(X_train)\n",
    "X_val_p   = preprocess_array(X_val)\n",
    "X_test_p  = preprocess_array(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431d31e",
   "metadata": {},
   "source": [
    "## Common evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "012d9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary(y_true, y_pred, name=\"model\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    print(f\"{name}: acc={acc:.4f} precision={p:.4f} recall={r:.4f} f1={f1:.4f}\")\n",
    "    return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c06e1",
   "metadata": {},
   "source": [
    "## TF-IDF + Logistic Regression (baseline + cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd5e3f",
   "metadata": {},
   "source": [
    "### Vectorize TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a83698c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((162000, 50000), (18000, 50000))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=cfg.max_features,\n",
    "    ngram_range=cfg.ngram_range,\n",
    "    min_df=cfg.min_df\n",
    ")\n",
    "\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train_p)\n",
    "Xva_tfidf = tfidf.transform(X_val_p)\n",
    "Xte_tfidf = tfidf.transform(X_test_p)\n",
    "\n",
    "Xtr_tfidf.shape, Xva_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccc9cb",
   "metadata": {},
   "source": [
    "### Logistic Regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "826dce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Stjepan\\Fax\\NepredniML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF+LogReg (val): acc=0.9043 precision=0.4405 recall=0.7311 f1=0.5498\n",
      "TFIDF+LogReg (test): acc=0.9038 precision=0.4401 recall=0.7486 f1=0.5543\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\"  # useful for skew\n",
    ")\n",
    "\n",
    "logreg.fit(Xtr_tfidf, y_train)\n",
    "\n",
    "val_pred = logreg.predict(Xva_tfidf)\n",
    "test_pred = logreg.predict(Xte_tfidf)\n",
    "\n",
    "metrics_logreg_val = eval_binary(y_val, val_pred, \"TFIDF+LogReg (val)\")\n",
    "metrics_logreg_test = eval_binary(y_test, test_pred, \"TFIDF+LogReg (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b6952",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "323efb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Stjepan\\Fax\\NepredniML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Stjepan\\Fax\\NepredniML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Stjepan\\Fax\\NepredniML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Stjepan\\Fax\\NepredniML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Stjepan\\Fax\\NepredniML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 scores: [0.53827288 0.55464664 0.54797413 0.54752148 0.54862385]\n",
      "CV F1 mean/std: 0.5474077973040654 0.005247094867706876\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=cfg.seed)\n",
    "cv_scores = cross_val_score(logreg, tfidf.transform(X_trainval), y_trainval, cv=skf, scoring=\"f1\")\n",
    "print(\"CV F1 scores:\", cv_scores)\n",
    "print(\"CV F1 mean/std:\", cv_scores.mean(), cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f1750",
   "metadata": {},
   "source": [
    "## TF-IDF + MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca38331",
   "metadata": {},
   "source": [
    "### Sparse TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a22e9294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162000, 50000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SparseTfidfDataset(Dataset):\n",
    "    def __init__(self, X_csr, y):\n",
    "        self.X = X_csr.tocsr()\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X[idx]\n",
    "        x = torch.from_numpy(row.toarray().ravel()).float()\n",
    "        y = torch.tensor(self.y[idx]).float()\n",
    "        return x, y\n",
    "\n",
    "train_ds = SparseTfidfDataset(Xtr_tfidf, y_train)\n",
    "val_ds   = SparseTfidfDataset(Xva_tfidf, y_val)\n",
    "test_ds  = SparseTfidfDataset(Xte_tfidf, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "Xtr_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e6d37",
   "metadata": {},
   "source": [
    "### MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01fc22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, dropout=0.3, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.activation = activation.lower()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "\n",
    "        # weight init\n",
    "        if self.activation == \"relu\":\n",
    "            nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.fc1(x))\n",
    "        if self.activation == \"relu\":\n",
    "            x = F.relu(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            x = torch.tanh(x)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(\"activation must be relu/tanh/sigmoid\")\n",
    "\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x).squeeze(1)  # logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99513b68",
   "metadata": {},
   "source": [
    "### Train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23ad32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model):\n",
    "    if cfg.optimizer.lower() == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    if cfg.optimizer.lower() == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
    "    raise ValueError(\"optimizer must be adam/sgd\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true.astype(int), y_prob, y_pred\n",
    "\n",
    "def fit_binary(model, train_loader, val_loader, epochs=3):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    opt = make_optimizer(model)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        yv, pv, yv_pred = eval_loader(model, val_loader)\n",
    "        m = eval_binary(yv, yv_pred, f\"MLP val ep{ep}\")\n",
    "        print(f\"Epoch {ep}/{epochs} train_loss={total_loss/n:.4f}\")\n",
    "\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1 = m[\"f1\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43928f",
   "metadata": {},
   "source": [
    "### Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d822cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP val ep1: acc=0.9403 precision=0.7593 recall=0.3704 f1=0.4979\n",
      "Epoch 1/3 train_loss=0.2189\n",
      "MLP val ep2: acc=0.9379 precision=0.6882 recall=0.4079 f1=0.5122\n",
      "Epoch 2/3 train_loss=0.1179\n",
      "MLP val ep3: acc=0.9358 precision=0.6336 recall=0.4663 f1=0.5372\n",
      "Epoch 3/3 train_loss=0.0756\n",
      "TFIDF+MLP (test): acc=0.9356 precision=0.6312 recall=0.4678 f1=0.5374\n"
     ]
    }
   ],
   "source": [
    "cfg.optimizer = \"adam\"\n",
    "cfg.lr = 1e-3\n",
    "cfg.epochs = 3\n",
    "\n",
    "mlp = TfidfMLP(in_dim=Xtr_tfidf.shape[1], hidden=256, dropout=0.3, activation=\"relu\")\n",
    "mlp = fit_binary(mlp, train_loader, val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yt, pt, ypred = eval_loader(mlp, test_loader)\n",
    "metrics_mlp_test = eval_binary(yt, ypred, \"TFIDF+MLP (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e284de",
   "metadata": {},
   "source": [
    "### Activation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71eb42c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP val ep1: acc=0.9387 precision=0.7417 recall=0.3572 f1=0.4822\n",
      "Epoch 1/2 train_loss=0.2199\n",
      "MLP val ep2: acc=0.9387 precision=0.6995 recall=0.4093 f1=0.5164\n",
      "Epoch 2/2 train_loss=0.1172\n",
      "TFIDF+MLP act=relu (test): acc=0.9392 precision=0.7009 recall=0.4178 f1=0.5235\n",
      "MLP val ep1: acc=0.9396 precision=0.7230 recall=0.3954 f1=0.5112\n",
      "Epoch 1/2 train_loss=0.3069\n",
      "MLP val ep2: acc=0.9384 precision=0.6901 recall=0.4163 f1=0.5193\n",
      "Epoch 2/2 train_loss=0.1330\n",
      "TFIDF+MLP act=tanh (test): acc=0.9385 precision=0.6855 recall=0.4253 f1=0.5249\n",
      "MLP val ep1: acc=0.9397 precision=0.7736 recall=0.3468 f1=0.4789\n",
      "Epoch 1/2 train_loss=0.2145\n",
      "MLP val ep2: acc=0.9411 precision=0.7342 recall=0.4128 f1=0.5285\n",
      "Epoch 2/2 train_loss=0.1482\n",
      "TFIDF+MLP act=sigmoid (test): acc=0.9403 precision=0.7170 recall=0.4184 f1=0.5284\n"
     ]
    }
   ],
   "source": [
    "for act in [\"relu\", \"tanh\", \"sigmoid\"]:\n",
    "    m = TfidfMLP(in_dim=Xtr_tfidf.shape[1], hidden=256, dropout=0.3, activation=act)\n",
    "    m = fit_binary(m, train_loader, val_loader, epochs=2)\n",
    "    yt, pt, ypred = eval_loader(m, test_loader)\n",
    "    eval_binary(yt, ypred, f\"TFIDF+MLP act={act} (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495e989",
   "metadata": {},
   "source": [
    "### Compare SGD vs Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97baaed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP val ep1: acc=0.9420 precision=0.7411 recall=0.4218 f1=0.5376\n",
      "Epoch 1/2 train_loss=0.1893\n",
      "MLP val ep2: acc=0.9361 precision=0.6289 recall=0.4899 f1=0.5508\n",
      "Epoch 2/2 train_loss=0.1008\n",
      "TFIDF+MLP SGD (test): acc=0.9372 precision=0.6324 recall=0.5122 f1=0.5660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9372,\n",
       " 'precision': 0.6324324324324324,\n",
       " 'recall': 0.5121951219512195,\n",
       " 'f1': 0.5659986178299931}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.optimizer = \"sgd\"\n",
    "cfg.lr = 0.05\n",
    "cfg.momentum = 0.9\n",
    "\n",
    "m = TfidfMLP(in_dim=Xtr_tfidf.shape[1], hidden=256, dropout=0.3, activation=\"relu\")\n",
    "m = fit_binary(m, train_loader, val_loader, epochs=2)\n",
    "yt, pt, ypred = eval_loader(m, test_loader)\n",
    "eval_binary(yt, ypred, \"TFIDF+MLP SGD (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddf157",
   "metadata": {},
   "source": [
    "## LSTM and BiLSTM+Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a8d7d",
   "metadata": {},
   "source": [
    "### Build vocabulary + encode sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de876f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_vocab: 100%|██████████| 162000/162000 [00:02<00:00, 70844.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "\n",
    "def build_vocab(texts, vocab_size=50_000):\n",
    "    counter = Counter()\n",
    "    for t in tqdm(texts, desc=\"build_vocab\"):\n",
    "        counter.update(basic_tokenize(t))\n",
    "    most = counter.most_common(vocab_size - 2)\n",
    "    stoi = {\"<PAD>\": PAD, \"<UNK>\": UNK}\n",
    "    for i, (w, _) in enumerate(most, start=2):\n",
    "        stoi[w] = i\n",
    "    return stoi\n",
    "\n",
    "stoi = build_vocab(X_train_p, vocab_size=cfg.vocab_size)\n",
    "itos_size = len(stoi)\n",
    "itos_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b2c5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, stoi, max_len=200):\n",
    "    toks = basic_tokenize(text)\n",
    "    ids = [stoi.get(w, UNK) for w in toks[:max_len]]\n",
    "    if len(ids) == 0:\n",
    "        ids = [UNK]\n",
    "    return ids\n",
    "\n",
    "\n",
    "def pad_batch(batch_ids, pad_id=PAD):\n",
    "    lens = torch.tensor([max(1, len(x)) for x in batch_ids], dtype=torch.long)\n",
    "    maxlen = int(lens.max().item())\n",
    "    padded = torch.full((len(batch_ids), maxlen), pad_id, dtype=torch.long)\n",
    "\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        if len(ids) == 0:\n",
    "            ids = [UNK]\n",
    "        padded[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    return padded, lens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d2db1",
   "metadata": {},
   "source": [
    "### Sequence Dataset + collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6258a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, texts, labels, stoi, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype(np.float32)\n",
    "        self.stoi = stoi\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = encode_text(self.texts[idx], self.stoi, self.max_len)\n",
    "        y = self.labels[idx]\n",
    "        return ids, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ids_list, ys = zip(*batch)\n",
    "    x, lens = pad_batch(ids_list, pad_id=PAD)\n",
    "    y = torch.tensor(ys, dtype=torch.float32)\n",
    "    return x, lens, y\n",
    "\n",
    "seq_train = SeqDataset(X_train_p, y_train, stoi, cfg.seq_max_len)\n",
    "seq_val   = SeqDataset(X_val_p, y_val, stoi, cfg.seq_max_len)\n",
    "seq_test  = SeqDataset(X_test_p, y_test, stoi, cfg.seq_max_len)\n",
    "\n",
    "seq_train_loader = DataLoader(seq_train, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "seq_val_loader   = DataLoader(seq_val, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "seq_test_loader  = DataLoader(seq_test, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ad659",
   "metadata": {},
   "source": [
    "### LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd649652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, layers=1, dropout=0.3, bidir=False):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden, num_layers=layers,\n",
    "            batch_first=True, dropout=dropout if layers > 1 else 0.0,\n",
    "            bidirectional=bidir\n",
    "        )\n",
    "        out_dim = hidden * (2 if bidir else 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        emb = self.emb(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed)\n",
    "\n",
    "        # last layer hidden\n",
    "        if self.lstm.bidirectional:\n",
    "            h_last = torch.cat([h[-2], h[-1]], dim=1)\n",
    "        else:\n",
    "            h_last = h[-1]\n",
    "\n",
    "        h_last = self.drop(h_last)\n",
    "        logits = self.fc(h_last).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692120a",
   "metadata": {},
   "source": [
    "### Train/eval loops for sequence models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "304526b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_seq(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, lens, y in loader:\n",
    "        x, lens = x.to(device), lens.to(device)\n",
    "        logits = model(x, lens)\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys).astype(int)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true, y_prob, y_pred\n",
    "\n",
    "def fit_seq(model, train_loader, val_loader, epochs=3):\n",
    "    model = model.to(device)\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    opt = make_optimizer(model)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, lens, y in train_loader:\n",
    "            x, lens, y = x.to(device), lens.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, lens)\n",
    "            loss = crit(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        yv, pv, yv_pred = eval_seq(model, val_loader)\n",
    "        m = eval_binary(yv, yv_pred, f\"SEQ val ep{ep}\")\n",
    "        print(f\"Epoch {ep}/{epochs} train_loss={total_loss/n:.4f}\")\n",
    "\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1 = m[\"f1\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe1206",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c8bd78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ val ep1: acc=0.9253 precision=0.7714 recall=0.0938 f1=0.1673\n",
      "Epoch 1/3 train_loss=0.2748\n",
      "SEQ val ep2: acc=0.9366 precision=0.6472 recall=0.4538 f1=0.5335\n",
      "Epoch 2/3 train_loss=0.2050\n",
      "SEQ val ep3: acc=0.9420 precision=0.7778 recall=0.3843 f1=0.5144\n",
      "Epoch 3/3 train_loss=0.1709\n",
      "LSTM (test): acc=0.9356 precision=0.6402 recall=0.4440 f1=0.5244\n"
     ]
    }
   ],
   "source": [
    "cfg.optimizer = \"adam\"\n",
    "cfg.lr = 1e-3\n",
    "cfg.epochs = 3\n",
    "cfg.grad_clip_norm = 1.0  # very relevant for RNN/LSTM\n",
    "\n",
    "lstm = LSTMClassifier(vocab_size=len(stoi), emb_dim=cfg.emb_dim, hidden=cfg.lstm_hidden,\n",
    "                      layers=cfg.lstm_layers, dropout=cfg.dropout, bidir=False)\n",
    "lstm = fit_seq(lstm, seq_train_loader, seq_val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yt, pt, ypred = eval_seq(lstm, seq_test_loader)\n",
    "metrics_lstm_test = eval_binary(yt, ypred, \"LSTM (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a6d53",
   "metadata": {},
   "source": [
    "## BiLSTM + Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb11b0",
   "metadata": {},
   "source": [
    "### Additive attention on token outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbad7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden, num_layers=layers,\n",
    "            batch_first=True, dropout=dropout if layers > 1 else 0.0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.attn = nn.Linear(hidden * 2, 1)  # score each timestep\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden * 2, 1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        emb = self.emb(x)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B,T,H*2)\n",
    "\n",
    "        # mask padding\n",
    "        B, T, _ = out.shape\n",
    "        mask = torch.arange(T, device=lens.device).unsqueeze(0) >= lens.unsqueeze(1)  # (B,T) True for pad\n",
    "\n",
    "        scores = self.attn(out).squeeze(-1)  # (B,T)\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "        weights = torch.softmax(scores, dim=1)  # (B,T)\n",
    "\n",
    "        context = torch.bmm(weights.unsqueeze(1), out).squeeze(1)  # (B,H*2)\n",
    "        context = self.drop(context)\n",
    "        logits = self.fc(context).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58842f68",
   "metadata": {},
   "source": [
    "### Train attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "894e25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ val ep1: acc=0.9387 precision=0.7530 recall=0.3475 f1=0.4755\n",
      "Epoch 1/3 train_loss=0.2407\n",
      "SEQ val ep2: acc=0.9442 precision=0.7686 recall=0.4315 f1=0.5527\n",
      "Epoch 2/3 train_loss=0.1817\n",
      "SEQ val ep3: acc=0.9452 precision=0.7475 recall=0.4753 f1=0.5811\n",
      "Epoch 3/3 train_loss=0.1637\n",
      "BiLSTM+Attention (test): acc=0.9444 precision=0.7387 recall=0.4703 f1=0.5747\n"
     ]
    }
   ],
   "source": [
    "attn_model = BiLSTMAttention(vocab_size=len(stoi), emb_dim=cfg.emb_dim, hidden=cfg.lstm_hidden,\n",
    "                            layers=cfg.lstm_layers, dropout=cfg.dropout)\n",
    "attn_model = fit_seq(attn_model, seq_train_loader, seq_val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yt, pt, ypred = eval_seq(attn_model, seq_test_loader)\n",
    "metrics_attn_test = eval_binary(yt, ypred, \"BiLSTM+Attention (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9a95a",
   "metadata": {},
   "source": [
    "## Autoencoder (MSE) on TF-IDF + classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacab13",
   "metadata": {},
   "source": [
    "### Autoencoder on TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2031e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfAutoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck=256):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, bottleneck),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(bottleneck, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, in_dim)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        xhat = self.dec(z)\n",
    "        return z, xhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a865117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller TF-IDF for AE to keep RAM reasonable\n",
    "tfidf_ae = TfidfVectorizer(max_features=20_000, ngram_range=(1,2), min_df=2)\n",
    "Xtr_ae = tfidf_ae.fit_transform(X_train_p)\n",
    "Xva_ae = tfidf_ae.transform(X_val_p)\n",
    "Xte_ae = tfidf_ae.transform(X_test_p)\n",
    "\n",
    "ae_train = SparseTfidfDataset(Xtr_ae, y_train)\n",
    "ae_val   = SparseTfidfDataset(Xva_ae, y_val)\n",
    "ae_test  = SparseTfidfDataset(Xte_ae, y_test)\n",
    "\n",
    "ae_train_loader = DataLoader(ae_train, batch_size=256, shuffle=True)\n",
    "ae_val_loader   = DataLoader(ae_val, batch_size=256, shuffle=False)\n",
    "ae_test_loader  = DataLoader(ae_test, batch_size=256, shuffle=False)\n",
    "\n",
    "in_dim = Xtr_ae.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73aa54ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE epoch 1/3 mse=0.000049\n",
      "AE epoch 2/3 mse=0.000049\n",
      "AE epoch 3/3 mse=0.000049\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(ae, loader, epochs=3):\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        n = 0\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            z, xhat = ae(x)\n",
    "            loss = loss_fn(xhat, x)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * x.size(0)\n",
    "            n += x.size(0)\n",
    "        print(f\"AE epoch {ep}/{epochs} mse={total/n:.6f}\")\n",
    "\n",
    "    return ae\n",
    "\n",
    "ae = TfidfAutoencoder(in_dim=in_dim, bottleneck=256)\n",
    "ae = train_autoencoder(ae, ae_train_loader, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e0715",
   "metadata": {},
   "source": [
    "### Encode TF-IDF through AE and train a classifier on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4076b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE-Embeddings + LogReg (test): acc=0.9201 precision=0.0000 recall=0.0000 f1=0.0000\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def encode_dataset(ae, loader):\n",
    "    ae.eval()\n",
    "    Z, Y = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        z, _ = ae(x)\n",
    "        Z.append(z.cpu().numpy())\n",
    "        Y.append(y.numpy())\n",
    "    return np.vstack(Z), np.concatenate(Y).astype(int)\n",
    "\n",
    "Ztr, Ytr = encode_dataset(ae, ae_train_loader)\n",
    "Zva, Yva = encode_dataset(ae, ae_val_loader)\n",
    "Zte, Yte = encode_dataset(ae, ae_test_loader)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, class_weight=\"balanced\")\n",
    "clf.fit(Ztr, Ytr)\n",
    "\n",
    "pred = clf.predict(Zte)\n",
    "metrics_ae_test = eval_binary(Yte, pred, \"AE-Embeddings + LogReg (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac0ca6",
   "metadata": {},
   "source": [
    "## Final comparison table - all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da103500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiLSTM+Attention</td>\n",
       "      <td>0.94435</td>\n",
       "      <td>0.738703</td>\n",
       "      <td>0.470294</td>\n",
       "      <td>0.574704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF+LogReg</td>\n",
       "      <td>0.90375</td>\n",
       "      <td>0.440074</td>\n",
       "      <td>0.748593</td>\n",
       "      <td>0.554295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF+MLP</td>\n",
       "      <td>0.93560</td>\n",
       "      <td>0.631224</td>\n",
       "      <td>0.467792</td>\n",
       "      <td>0.537356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93560</td>\n",
       "      <td>0.640216</td>\n",
       "      <td>0.444028</td>\n",
       "      <td>0.524372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE+LogReg</td>\n",
       "      <td>0.92005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      acc  precision    recall        f1\n",
       "3  BiLSTM+Attention  0.94435   0.738703  0.470294  0.574704\n",
       "0      TFIDF+LogReg  0.90375   0.440074  0.748593  0.554295\n",
       "1         TFIDF+MLP  0.93560   0.631224  0.467792  0.537356\n",
       "2              LSTM  0.93560   0.640216  0.444028  0.524372\n",
       "4         AE+LogReg  0.92005   0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "def add(name, metrics):\n",
    "    rows.append({\"model\": name, **metrics})\n",
    "\n",
    "add(\"TFIDF+LogReg\", metrics_logreg_test)\n",
    "add(\"TFIDF+MLP\", metrics_mlp_test)\n",
    "add(\"LSTM\", metrics_lstm_test)\n",
    "add(\"BiLSTM+Attention\", metrics_attn_test)\n",
    "add(\"AE+LogReg\", metrics_ae_test)\n",
    "\n",
    "pd.DataFrame(rows).sort_values(\"f1\", ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

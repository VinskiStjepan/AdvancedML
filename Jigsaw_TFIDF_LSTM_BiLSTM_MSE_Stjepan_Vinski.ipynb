{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c61a8b5",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc97645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U pandas numpy scikit-learn scipy matplotlib tqdm nltk\n",
    "#%pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f9bcf",
   "metadata": {},
   "source": [
    "## Imports + config + device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae9b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1+cu121\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "import os, re, random, time\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff2c41",
   "metadata": {},
   "source": [
    "## Reproducibility + global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0cfca64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(seed=42, data_path='./data/jigsaw/train.csv', subsample_n=200000, test_size=0.1, val_size=0.1, label_threshold=0.5, max_features=50000, ngram_range=(1, 2), min_df=2, batch_size=256, epochs=3, lr=0.001, weight_decay=0.0001, optimizer='adam', momentum=0.9, grad_clip_norm=1.0, seq_max_len=200, vocab_size=50000, emb_dim=128, lstm_hidden=128, lstm_layers=1, dropout=0.3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    data_path: str = \"./data/jigsaw/train.csv\"\n",
    "    subsample_n: int | None = 200_000  # set None to use full file (big)\n",
    "    test_size: float = 0.1\n",
    "    val_size: float = 0.1  # fraction of remaining train for validation\n",
    "    label_threshold: float = 0.5\n",
    "\n",
    "    # TF-IDF\n",
    "    max_features: int = 50_000\n",
    "    ngram_range: tuple = (1, 2)\n",
    "    min_df: int = 2\n",
    "\n",
    "    # PyTorch training\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 3\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    optimizer: str = \"adam\"  # \"adam\" or \"sgd\"\n",
    "    momentum: float = 0.9\n",
    "    grad_clip_norm: float | None = 1.0\n",
    "\n",
    "    # Sequence model\n",
    "    seq_max_len: int = 200\n",
    "    vocab_size: int = 50_000\n",
    "    emb_dim: int = 128\n",
    "    lstm_hidden: int = 128\n",
    "    lstm_layers: int = 1\n",
    "    dropout: float = 0.3\n",
    "\n",
    "cfg = CFG()\n",
    "seed_everything(cfg.seed)\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3697cc",
   "metadata": {},
   "source": [
    "## Load Jigsaw train.csv + label + subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70fc5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.103228</td>\n",
       "      <td>0.079970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.197294</td>\n",
       "      <td>0.271247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          label\n",
       "count  200000.000000  200000.000000\n",
       "mean        0.103228       0.079970\n",
       "std         0.197294       0.271247\n",
       "min         0.000000       0.000000\n",
       "25%         0.000000       0.000000\n",
       "50%         0.000000       0.000000\n",
       "75%         0.166667       0.000000\n",
       "max         1.000000       1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(cfg.data_path)\n",
    "print(df.shape)\n",
    "df = df.dropna(subset=[\"comment_text\", \"target\"]).reset_index(drop=True)\n",
    "\n",
    "df[\"label\"] = (df[\"target\"] >= cfg.label_threshold).astype(int)\n",
    "\n",
    "# Optional: subsample (keep label distribution)\n",
    "if cfg.subsample_n is not None and len(df) > cfg.subsample_n:\n",
    "    df, _ = train_test_split(\n",
    "        df,\n",
    "        train_size=cfg.subsample_n,\n",
    "        stratify=df[\"label\"],\n",
    "        random_state=cfg.seed\n",
    "    )\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "df[[\"target\", \"label\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477226a",
   "metadata": {},
   "source": [
    "## Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81690083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 162000 Val: 18000 Test: 20000\n",
      "Pos rate train/val/test: 0.07997530864197532 0.07994444444444444 0.07995\n"
     ]
    }
   ],
   "source": [
    "X = df[\"comment_text\"].astype(str).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# First: split out test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=cfg.test_size,\n",
    "    stratify=y,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "# Then: split train/val from remaining\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval,\n",
    "    test_size=cfg.val_size,\n",
    "    stratify=y_trainval,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n",
    "print(\"Pos rate train/val/test:\", y_train.mean(), y_val.mean(), y_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d6fe1",
   "metadata": {},
   "source": [
    "## Preprocessing: tokenization + stemming + lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b0abd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "preprocess=raw: 100%|██████████| 162000/162000 [00:02<00:00, 60441.32it/s]\n",
      "preprocess=raw: 100%|██████████| 18000/18000 [00:00<00:00, 62636.82it/s]\n",
      "preprocess=raw: 100%|██████████| 20000/20000 [00:00<00:00, 60257.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# NLTK downloads (run once)\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "_url_re = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "_html_re = re.compile(r\"<.*?>\")\n",
    "_user_re = re.compile(r\"@\\w+\")\n",
    "_non_ascii_re = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "\n",
    "_token_re = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "CLEAN_TEXT = True\n",
    "REMOVE_STOPWORDS = True\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = _url_re.sub(\" \", text)\n",
    "    text = _html_re.sub(\" \", text)\n",
    "    text = _user_re.sub(\" \", text)\n",
    "    text = _non_ascii_re.sub(\" \", text)\n",
    "    return text\n",
    "\n",
    "def basic_tokenize(text: str):\n",
    "    if CLEAN_TEXT:\n",
    "        text = clean_text(text)\n",
    "    else:\n",
    "        text = text.lower()\n",
    "    toks = _token_re.findall(text)\n",
    "    if REMOVE_STOPWORDS:\n",
    "        toks = [t for t in toks if t not in stop_words]\n",
    "    return toks\n",
    "\n",
    "def stem_text(text: str) -> str:\n",
    "    toks = basic_tokenize(text)\n",
    "    toks = [stemmer.stem(t) for t in toks]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def lemma_text(text: str) -> str:\n",
    "    toks = basic_tokenize(text)\n",
    "    toks = [lemmatizer.lemmatize(t) for t in toks]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Choose one:\n",
    "PREPROC_MODE = \"raw\"  # \"raw\" | \"stem\" | \"lemma\"\n",
    "\n",
    "def preprocess_array(arr):\n",
    "    if PREPROC_MODE == \"raw\":\n",
    "        if CLEAN_TEXT or REMOVE_STOPWORDS:\n",
    "            return np.array([\" \".join(basic_tokenize(x)) for x in tqdm(arr, desc=f\"preprocess={PREPROC_MODE}\")], dtype=object)\n",
    "        return arr\n",
    "    fn = stem_text if PREPROC_MODE == \"stem\" else lemma_text\n",
    "    return np.array([fn(x) for x in tqdm(arr, desc=f\"preprocess={PREPROC_MODE}\")], dtype=object)\n",
    "X_train_p = preprocess_array(X_train)\n",
    "X_val_p   = preprocess_array(X_val)\n",
    "X_test_p  = preprocess_array(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431d31e",
   "metadata": {},
   "source": [
    "## Common evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "012d9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary(y_true, y_pred, name=\"model\", show_report=False):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    print(f\"{name}: acc={acc:.4f} precision={p:.4f} recall={r:.4f} f1={f1:.4f}\")\n",
    "    if show_report:\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        print(\"Classification report:\")\n",
    "        print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f264fb2",
   "metadata": {},
   "source": [
    "## Bag‑of‑Words + LogReg (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW+LogReg (val): acc=0.9147 precision=0.4753 recall=0.6484 f1=0.5485\n",
      "BOW+LogReg (test): acc=0.9193 precision=0.4964 recall=0.6842 f1=0.5753\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(\n",
    "    max_features=cfg.max_features,\n",
    "    ngram_range=cfg.ngram_range,\n",
    "    min_df=cfg.min_df\n",
    ")\n",
    "\n",
    "Xtr_bow = bow.fit_transform(X_train_p)\n",
    "Xva_bow = bow.transform(X_val_p)\n",
    "Xte_bow = bow.transform(X_test_p)\n",
    "\n",
    "logreg_bow = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "logreg_bow.fit(Xtr_bow, y_train)\n",
    "\n",
    "val_pred = logreg_bow.predict(Xva_bow)\n",
    "test_pred = logreg_bow.predict(Xte_bow)\n",
    "\n",
    "metrics_bow_val = eval_binary(y_val, val_pred, \"BOW+LogReg (val)\")\n",
    "metrics_bow_test = eval_binary(y_test, test_pred, \"BOW+LogReg (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c06e1",
   "metadata": {},
   "source": [
    "## TF-IDF + Logistic Regression (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd5e3f",
   "metadata": {},
   "source": [
    "### Vectorize TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a83698c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((162000, 50000), (18000, 50000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=cfg.max_features,\n",
    "    ngram_range=cfg.ngram_range,\n",
    "    min_df=cfg.min_df\n",
    ")\n",
    "\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train_p)\n",
    "Xva_tfidf = tfidf.transform(X_val_p)\n",
    "Xte_tfidf = tfidf.transform(X_test_p)\n",
    "\n",
    "Xtr_tfidf.shape, Xva_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccc9cb",
   "metadata": {},
   "source": [
    "### Logistic Regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "826dce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF+LogReg (val): acc=0.9057 precision=0.4460 recall=0.7408 f1=0.5568\n",
      "TFIDF+LogReg (test): acc=0.9080 precision=0.4554 recall=0.7699 f1=0.5723\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\"  # useful for skew\n",
    ")\n",
    "\n",
    "logreg.fit(Xtr_tfidf, y_train)\n",
    "\n",
    "val_pred = logreg.predict(Xva_tfidf)\n",
    "test_pred = logreg.predict(Xte_tfidf)\n",
    "\n",
    "metrics_logreg_val = eval_binary(y_val, val_pred, \"TFIDF+LogReg (val)\")\n",
    "metrics_logreg_test = eval_binary(y_test, test_pred, \"TFIDF+LogReg (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f1750",
   "metadata": {},
   "source": [
    "## TF-IDF + MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca38331",
   "metadata": {},
   "source": [
    "### Sparse TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a22e9294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162000, 50000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SparseTfidfDataset(Dataset):\n",
    "    def __init__(self, X_csr, y):\n",
    "        self.X = X_csr.tocsr()\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X[idx]\n",
    "        x = torch.from_numpy(row.toarray().ravel()).float()\n",
    "        y = torch.tensor(self.y[idx]).float()\n",
    "        return x, y\n",
    "\n",
    "train_ds = SparseTfidfDataset(Xtr_tfidf, y_train)\n",
    "val_ds   = SparseTfidfDataset(Xva_tfidf, y_val)\n",
    "test_ds  = SparseTfidfDataset(Xte_tfidf, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "Xtr_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e6d37",
   "metadata": {},
   "source": [
    "### MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01fc22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, dropout=0.3, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.activation = activation.lower()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "\n",
    "        # weight init\n",
    "        if self.activation == \"relu\":\n",
    "            nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.fc1(x))\n",
    "        if self.activation == \"relu\":\n",
    "            x = F.relu(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            x = torch.tanh(x)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(\"activation must be relu/tanh/sigmoid\")\n",
    "\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x).squeeze(1)  # logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99513b68",
   "metadata": {},
   "source": [
    "### Train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23ad32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model):\n",
    "    if cfg.optimizer.lower() == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    if cfg.optimizer.lower() == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
    "    raise ValueError(\"optimizer must be adam/sgd\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true.astype(int), y_prob, y_pred\n",
    "\n",
    "def fit_binary(model, train_loader, val_loader, epochs=3):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    opt = make_optimizer(model)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        yv, pv, yv_pred = eval_loader(model, val_loader)\n",
    "        m = eval_binary(yv, yv_pred, f\"MLP val ep{ep}\")\n",
    "        print(f\"Epoch {ep}/{epochs} train_loss={total_loss/n:.4f}\")\n",
    "\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1 = m[\"f1\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43928f",
   "metadata": {},
   "source": [
    "### Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d822cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP val ep1: acc=0.9406 precision=0.7387 recall=0.3968 f1=0.5163\n",
      "Epoch 1/3 train_loss=0.2128\n",
      "MLP val ep2: acc=0.9386 precision=0.6951 recall=0.4135 f1=0.5185\n",
      "Epoch 2/3 train_loss=0.1166\n",
      "MLP val ep3: acc=0.9356 precision=0.6346 recall=0.4587 f1=0.5325\n",
      "Epoch 3/3 train_loss=0.0754\n",
      "TFIDF+MLP (test): acc=0.9399 precision=0.6653 recall=0.5009 f1=0.5715\n"
     ]
    }
   ],
   "source": [
    "cfg.optimizer = \"adam\"\n",
    "cfg.lr = 1e-3\n",
    "cfg.epochs = 3\n",
    "\n",
    "mlp = TfidfMLP(in_dim=Xtr_tfidf.shape[1], hidden=256, dropout=0.3, activation=\"relu\")\n",
    "mlp = fit_binary(mlp, train_loader, val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yt, pt, ypred = eval_loader(mlp, test_loader)\n",
    "metrics_mlp_test = eval_binary(yt, ypred, \"TFIDF+MLP (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddf157",
   "metadata": {},
   "source": [
    "## LSTM and BiLSTM+Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a8d7d",
   "metadata": {},
   "source": [
    "### Build vocabulary + encode sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de876f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_vocab:   0%|          | 0/162000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_vocab: 100%|██████████| 162000/162000 [00:02<00:00, 68848.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "\n",
    "def build_vocab(texts, vocab_size=50_000):\n",
    "    counter = Counter()\n",
    "    for t in tqdm(texts, desc=\"build_vocab\"):\n",
    "        counter.update(basic_tokenize(t))\n",
    "    most = counter.most_common(vocab_size - 2)\n",
    "    stoi = {\"<PAD>\": PAD, \"<UNK>\": UNK}\n",
    "    for i, (w, _) in enumerate(most, start=2):\n",
    "        stoi[w] = i\n",
    "    return stoi\n",
    "\n",
    "stoi = build_vocab(X_train_p, vocab_size=cfg.vocab_size)\n",
    "itos_size = len(stoi)\n",
    "itos_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b2c5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, stoi, max_len=200):\n",
    "    toks = basic_tokenize(text)\n",
    "    ids = [stoi.get(w, UNK) for w in toks[:max_len]]\n",
    "    if len(ids) == 0:\n",
    "        ids = [UNK]\n",
    "    return ids\n",
    "\n",
    "\n",
    "def pad_batch(batch_ids, pad_id=PAD):\n",
    "    lens = torch.tensor([max(1, len(x)) for x in batch_ids], dtype=torch.long)\n",
    "    maxlen = int(lens.max().item())\n",
    "    padded = torch.full((len(batch_ids), maxlen), pad_id, dtype=torch.long)\n",
    "\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        if len(ids) == 0:\n",
    "            ids = [UNK]\n",
    "        padded[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    return padded, lens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d2db1",
   "metadata": {},
   "source": [
    "### Sequence Dataset + collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6258a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, texts, labels, stoi, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype(np.float32)\n",
    "        self.stoi = stoi\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = encode_text(self.texts[idx], self.stoi, self.max_len)\n",
    "        y = self.labels[idx]\n",
    "        return ids, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ids_list, ys = zip(*batch)\n",
    "    x, lens = pad_batch(ids_list, pad_id=PAD)\n",
    "    y = torch.tensor(ys, dtype=torch.float32)\n",
    "    return x, lens, y\n",
    "\n",
    "seq_train = SeqDataset(X_train_p, y_train, stoi, cfg.seq_max_len)\n",
    "seq_val   = SeqDataset(X_val_p, y_val, stoi, cfg.seq_max_len)\n",
    "seq_test  = SeqDataset(X_test_p, y_test, stoi, cfg.seq_max_len)\n",
    "\n",
    "seq_train_loader = DataLoader(seq_train, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "seq_val_loader   = DataLoader(seq_val, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "seq_test_loader  = DataLoader(seq_test, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ad659",
   "metadata": {},
   "source": [
    "### LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd649652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, layers=1, dropout=0.3, bidir=False):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden, num_layers=layers,\n",
    "            batch_first=True, dropout=dropout if layers > 1 else 0.0,\n",
    "            bidirectional=bidir\n",
    "        )\n",
    "        out_dim = hidden * (2 if bidir else 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        emb = self.emb(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed)\n",
    "\n",
    "        # last layer hidden\n",
    "        if self.lstm.bidirectional:\n",
    "            h_last = torch.cat([h[-2], h[-1]], dim=1)\n",
    "        else:\n",
    "            h_last = h[-1]\n",
    "\n",
    "        h_last = self.drop(h_last)\n",
    "        logits = self.fc(h_last).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692120a",
   "metadata": {},
   "source": [
    "### Train/eval loops for sequence models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "304526b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_seq(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, lens, y in loader:\n",
    "        x, lens = x.to(device), lens.to(device)\n",
    "        logits = model(x, lens)\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys).astype(int)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true, y_prob, y_pred\n",
    "\n",
    "def fit_seq(model, train_loader, val_loader, epochs=3):\n",
    "    model = model.to(device)\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    opt = make_optimizer(model)\n",
    "\n",
    "    best_f1 = -1\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, lens, y in train_loader:\n",
    "            x, lens, y = x.to(device), lens.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, lens)\n",
    "            loss = crit(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        yv, pv, yv_pred = eval_seq(model, val_loader)\n",
    "        m = eval_binary(yv, yv_pred, f\"SEQ val ep{ep}\")\n",
    "        print(f\"Epoch {ep}/{epochs} train_loss={total_loss/n:.4f}\")\n",
    "\n",
    "        if m[\"f1\"] > best_f1:\n",
    "            best_f1 = m[\"f1\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe1206",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c8bd78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ val ep1: acc=0.9282 precision=0.7829 recall=0.1404 f1=0.2381\n",
      "Epoch 1/3 train_loss=0.2686\n",
      "SEQ val ep2: acc=0.9378 precision=0.7243 recall=0.3579 f1=0.4791\n",
      "Epoch 2/3 train_loss=0.2038\n",
      "SEQ val ep3: acc=0.9433 precision=0.7696 recall=0.4156 f1=0.5397\n",
      "Epoch 3/3 train_loss=0.1711\n",
      "LSTM (test): acc=0.9416 precision=0.7575 recall=0.3965 f1=0.5205\n"
     ]
    }
   ],
   "source": [
    "cfg.optimizer = \"adam\"\n",
    "cfg.lr = 1e-3\n",
    "cfg.epochs = 3\n",
    "cfg.grad_clip_norm = 1.0  # very relevant for RNN/LSTM\n",
    "\n",
    "lstm = LSTMClassifier(vocab_size=len(stoi), emb_dim=cfg.emb_dim, hidden=cfg.lstm_hidden,\n",
    "                      layers=cfg.lstm_layers, dropout=cfg.dropout, bidir=False)\n",
    "lstm = fit_seq(lstm, seq_train_loader, seq_val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yt, pt, ypred = eval_seq(lstm, seq_test_loader)\n",
    "metrics_lstm_test = eval_binary(yt, ypred, \"LSTM (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a6d53",
   "metadata": {},
   "source": [
    "## BiLSTM + Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb11b0",
   "metadata": {},
   "source": [
    "### Additive attention on token outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbad7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden, num_layers=layers,\n",
    "            batch_first=True, dropout=dropout if layers > 1 else 0.0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.attn = nn.Linear(hidden * 2, 1)  # score each timestep\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden * 2, 1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        emb = self.emb(x)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B,T,H*2)\n",
    "\n",
    "        # mask padding\n",
    "        B, T, _ = out.shape\n",
    "        mask = torch.arange(T, device=lens.device).unsqueeze(0) >= lens.unsqueeze(1)  # (B,T) True for pad\n",
    "\n",
    "        scores = self.attn(out).squeeze(-1)  # (B,T)\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "        weights = torch.softmax(scores, dim=1)  # (B,T)\n",
    "\n",
    "        context = torch.bmm(weights.unsqueeze(1), out).squeeze(1)  # (B,H*2)\n",
    "        context = self.drop(context)\n",
    "        logits = self.fc(context).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58842f68",
   "metadata": {},
   "source": [
    "### Train attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "894e25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ val ep1: acc=0.9388 precision=0.7363 recall=0.3648 f1=0.4879\n",
      "Epoch 1/3 train_loss=0.2363\n",
      "SEQ val ep2: acc=0.9414 precision=0.7207 recall=0.4357 f1=0.5431\n",
      "Epoch 2/3 train_loss=0.1832\n",
      "SEQ val ep3: acc=0.9449 precision=0.7738 recall=0.4399 f1=0.5609\n",
      "Epoch 3/3 train_loss=0.1651\n",
      "BiLSTM+Attention (test): acc=0.9441 precision=0.7780 recall=0.4209 f1=0.5463\n",
      "Confusion matrix:\n",
      "[[18209   192]\n",
      " [  926   673]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9516    0.9896    0.9702     18401\n",
      "           1     0.7780    0.4209    0.5463      1599\n",
      "\n",
      "    accuracy                         0.9441     20000\n",
      "   macro avg     0.8648    0.7052    0.7582     20000\n",
      "weighted avg     0.9377    0.9441    0.9363     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attn_model = BiLSTMAttention(vocab_size=len(stoi), emb_dim=cfg.emb_dim, hidden=cfg.lstm_hidden,\n",
    "                            layers=cfg.lstm_layers, dropout=cfg.dropout)\n",
    "attn_model = fit_seq(attn_model, seq_train_loader, seq_val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yt, pt, ypred = eval_seq(attn_model, seq_test_loader)\n",
    "metrics_attn_test = eval_binary(yt, ypred, \"BiLSTM+Attention (test)\", show_report=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9a95a",
   "metadata": {},
   "source": [
    "## Autoencoder (MSE) on TF-IDF + classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacab13",
   "metadata": {},
   "source": [
    "### Autoencoder on TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2031e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfAutoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck=256):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, bottleneck),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(bottleneck, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, in_dim)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        xhat = self.dec(z)\n",
    "        return z, xhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a865117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the TF‑IDF feature count to keep RAM reasonable\n",
    "tfidf_ae = TfidfVectorizer(max_features=20_000, ngram_range=(1,2), min_df=2)\n",
    "Xtr_ae = tfidf_ae.fit_transform(X_train_p)\n",
    "Xva_ae = tfidf_ae.transform(X_val_p)\n",
    "Xte_ae = tfidf_ae.transform(X_test_p)\n",
    "\n",
    "ae_train = SparseTfidfDataset(Xtr_ae, y_train)\n",
    "ae_val   = SparseTfidfDataset(Xva_ae, y_val)\n",
    "ae_test  = SparseTfidfDataset(Xte_ae, y_test)\n",
    "\n",
    "ae_train_loader = DataLoader(ae_train, batch_size=256, shuffle=True)\n",
    "ae_val_loader   = DataLoader(ae_val, batch_size=256, shuffle=False)\n",
    "ae_test_loader  = DataLoader(ae_test, batch_size=256, shuffle=False)\n",
    "\n",
    "in_dim = Xtr_ae.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73aa54ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE epoch 1/3 mse=0.000050\n",
      "AE epoch 2/3 mse=0.000050\n",
      "AE epoch 3/3 mse=0.000050\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(ae, loader, epochs=3):\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        n = 0\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            z, xhat = ae(x)\n",
    "            loss = loss_fn(xhat, x)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * x.size(0)\n",
    "            n += x.size(0)\n",
    "        print(f\"AE epoch {ep}/{epochs} mse={total/n:.6f}\")\n",
    "\n",
    "    return ae\n",
    "\n",
    "ae = TfidfAutoencoder(in_dim=in_dim, bottleneck=256)\n",
    "ae = train_autoencoder(ae, ae_train_loader, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e0715",
   "metadata": {},
   "source": [
    "### Encode TF-IDF through AE and train a classifier on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4076b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE-Embeddings + LogReg (test): acc=0.9201 precision=0.0000 recall=0.0000 f1=0.0000\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def encode_dataset(ae, loader):\n",
    "    ae.eval()\n",
    "    Z, Y = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        z, _ = ae(x)\n",
    "        Z.append(z.cpu().numpy())\n",
    "        Y.append(y.numpy())\n",
    "    return np.vstack(Z), np.concatenate(Y).astype(int)\n",
    "\n",
    "Ztr, Ytr = encode_dataset(ae, ae_train_loader)\n",
    "Zva, Yva = encode_dataset(ae, ae_val_loader)\n",
    "Zte, Yte = encode_dataset(ae, ae_test_loader)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, class_weight=\"balanced\")\n",
    "clf.fit(Ztr, Ytr)\n",
    "\n",
    "pred = clf.predict(Zte)\n",
    "metrics_ae_test = eval_binary(Yte, pred, \"AE-Embeddings + LogReg (test)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac0ca6",
   "metadata": {},
   "source": [
    "## Final comparison table - all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da103500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOW+LogReg</td>\n",
       "      <td>0.91925</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.684178</td>\n",
       "      <td>0.575335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF+LogReg</td>\n",
       "      <td>0.90800</td>\n",
       "      <td>0.455420</td>\n",
       "      <td>0.769856</td>\n",
       "      <td>0.572292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TFIDF+MLP</td>\n",
       "      <td>0.93995</td>\n",
       "      <td>0.665282</td>\n",
       "      <td>0.500938</td>\n",
       "      <td>0.571531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BiLSTM+Attention</td>\n",
       "      <td>0.94410</td>\n",
       "      <td>0.778035</td>\n",
       "      <td>0.420888</td>\n",
       "      <td>0.546266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.94160</td>\n",
       "      <td>0.757467</td>\n",
       "      <td>0.396498</td>\n",
       "      <td>0.520525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AE+LogReg</td>\n",
       "      <td>0.92005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model      acc  precision    recall        f1\n",
       "0        BOW+LogReg  0.91925   0.496370  0.684178  0.575335\n",
       "1      TFIDF+LogReg  0.90800   0.455420  0.769856  0.572292\n",
       "2         TFIDF+MLP  0.93995   0.665282  0.500938  0.571531\n",
       "4  BiLSTM+Attention  0.94410   0.778035  0.420888  0.546266\n",
       "3              LSTM  0.94160   0.757467  0.396498  0.520525\n",
       "5         AE+LogReg  0.92005   0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "def add(name, metrics):\n",
    "    rows.append({\"model\": name, **metrics})\n",
    "\n",
    "add(\"BOW+LogReg\", metrics_bow_test)\n",
    "add(\"TFIDF+LogReg\", metrics_logreg_test)\n",
    "add(\"TFIDF+MLP\", metrics_mlp_test)\n",
    "add(\"LSTM\", metrics_lstm_test)\n",
    "add(\"BiLSTM+Attention\", metrics_attn_test)\n",
    "add(\"AE+LogReg\", metrics_ae_test)\n",
    "\n",
    "pd.DataFrame(rows).sort_values(\"f1\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

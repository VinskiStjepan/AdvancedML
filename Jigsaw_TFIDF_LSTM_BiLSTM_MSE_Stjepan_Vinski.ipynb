{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c61a8b5",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc97645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U pandas numpy scikit-learn scipy matplotlib tqdm nltk\n",
    "#%pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install optuna optuna-dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f9bcf",
   "metadata": {},
   "source": [
    "## Imports + config + device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eae9b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1+cu121\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "import os, re, random, time\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, f1_score, average_precision_score, log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ff2c41",
   "metadata": {},
   "source": [
    "## Reproducibility + global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0cfca64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(seed=42, data_path='./data/jigsaw/train.csv', subsample_n=200000, test_size=0.1, val_size=0.1, label_threshold=0.5, max_features=50000, ngram_range=(1, 2), min_df=2, batch_size=256, epochs=3, lr=0.001, weight_decay=0.0001, optimizer='adam', momentum=0.9, grad_clip_norm=1.0, seq_max_len=200, vocab_size=50000, emb_dim=128, lstm_hidden=128, lstm_layers=1, dropout=0.3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    seed: int = 42\n",
    "    data_path: str = \"./data/jigsaw/train.csv\"\n",
    "    subsample_n: int | None = 200_000  # set None to use full file (big)\n",
    "    test_size: float = 0.1\n",
    "    val_size: float = 0.1  # fraction of remaining train for validation\n",
    "    label_threshold: float = 0.5\n",
    "\n",
    "    # TF-IDF\n",
    "    max_features: int = 50_000\n",
    "    ngram_range: tuple = (1, 2)\n",
    "    min_df: int = 2\n",
    "\n",
    "    # PyTorch training\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 3\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    optimizer: str = \"adam\"  # \"adam\" or \"sgd\"\n",
    "    momentum: float = 0.9\n",
    "    grad_clip_norm: float | None = 1.0\n",
    "\n",
    "    # Sequence model\n",
    "    seq_max_len: int = 200\n",
    "    vocab_size: int = 50_000\n",
    "    emb_dim: int = 128\n",
    "    lstm_hidden: int = 128\n",
    "    lstm_layers: int = 1\n",
    "    dropout: float = 0.3\n",
    "\n",
    "cfg = CFG()\n",
    "seed_everything(cfg.seed)\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3697cc",
   "metadata": {},
   "source": [
    "## Load Jigsaw train.csv + label + subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70fc5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 45)\n",
      "Class counts:\n",
      " label\n",
      "0    184006\n",
      "1     15994\n",
      "Name: count, dtype: int64\n",
      "Class ratios:\n",
      " label\n",
      "0    0.92\n",
      "1    0.08\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(cfg.data_path)\n",
    "print(df.shape)\n",
    "df = df.dropna(subset=[\"comment_text\", \"target\"]).reset_index(drop=True)\n",
    "\n",
    "df[\"label\"] = (df[\"target\"] >= cfg.label_threshold).astype(int)\n",
    "\n",
    "# Optional: subsample (keep label distribution)\n",
    "if cfg.subsample_n is not None and len(df) > cfg.subsample_n:\n",
    "    df, _ = train_test_split(\n",
    "        df,\n",
    "        train_size=cfg.subsample_n,\n",
    "        stratify=df[\"label\"],\n",
    "        random_state=cfg.seed\n",
    "    )\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "df[[\"target\", \"label\"]].describe()\n",
    "\n",
    "# Class frequency (counts and ratios)\n",
    "class_counts = df[\"label\"].value_counts().sort_index()\n",
    "class_ratios = (class_counts / class_counts.sum()).round(4)\n",
    "print(\"Class counts:\\n\", class_counts)\n",
    "print(\"Class ratios:\\n\", class_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477226a",
   "metadata": {},
   "source": [
    "## Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81690083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 162000 Val: 18000 Test: 20000\n",
      "Pos rate train/val/test: 0.07997530864197532 0.07994444444444444 0.07995\n",
      "Train counts:\n",
      "0    149044\n",
      "1     12956\n",
      "Name: count, dtype: int64\n",
      "Train ratios:\n",
      "0    0.92\n",
      "1    0.08\n",
      "Name: count, dtype: float64\n",
      "Val counts:\n",
      "0    16561\n",
      "1     1439\n",
      "Name: count, dtype: int64\n",
      "Val ratios:\n",
      "0    0.9201\n",
      "1    0.0799\n",
      "Name: count, dtype: float64\n",
      "Test counts:\n",
      "0    18401\n",
      "1     1599\n",
      "Name: count, dtype: int64\n",
      "Test ratios:\n",
      "0    0.9200\n",
      "1    0.0799\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = df[\"comment_text\"].astype(str).values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# First: split out test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=cfg.test_size,\n",
    "    stratify=y,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "# Then: split train/val from remaining\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval,\n",
    "    test_size=cfg.val_size,\n",
    "    stratify=y_trainval,\n",
    "    random_state=cfg.seed\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train), \"Val:\", len(X_val), \"Test:\", len(X_test))\n",
    "print(\"Pos rate train/val/test:\", y_train.mean(), y_val.mean(), y_test.mean())\n",
    "\n",
    "def show_split_stats(name, y):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    ratios = (counts / counts.sum()).round(4)\n",
    "    print(f\"{name} counts:\\n{counts}\")\n",
    "    print(f\"{name} ratios:\\n{ratios}\")\n",
    "\n",
    "show_split_stats(\"Train\", y_train)\n",
    "show_split_stats(\"Val\", y_val)\n",
    "show_split_stats(\"Test\", y_test)\n",
    "train_class_counts = np.bincount(y_train)\n",
    "train_class_weights = 1.0 / train_class_counts\n",
    "train_sample_weights = train_class_weights[y_train]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d6fe1",
   "metadata": {},
   "source": [
    "## Preprocessing: tokenization + stemming + lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b0abd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stjepan.vinski\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "preprocess=raw: 100%|██████████| 162000/162000 [00:02<00:00, 63732.20it/s]\n",
      "preprocess=raw: 100%|██████████| 18000/18000 [00:00<00:00, 64410.22it/s]\n",
      "preprocess=raw: 100%|██████████| 20000/20000 [00:00<00:00, 64494.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# NLTK downloads (run once)\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "_url_re = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "_html_re = re.compile(r\"<.*?>\")\n",
    "_user_re = re.compile(r\"@\\w+\")\n",
    "_non_ascii_re = re.compile(r\"[^\\x00-\\x7F]+\")\n",
    "\n",
    "_token_re = re.compile(r\"[A-Za-z']+\")\n",
    "\n",
    "CLEAN_TEXT = True\n",
    "REMOVE_STOPWORDS = True\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = _url_re.sub(\" \", text)\n",
    "    text = _html_re.sub(\" \", text)\n",
    "    text = _user_re.sub(\" \", text)\n",
    "    text = _non_ascii_re.sub(\" \", text)\n",
    "    return text\n",
    "\n",
    "def basic_tokenize(text: str):\n",
    "    if CLEAN_TEXT:\n",
    "        text = clean_text(text)\n",
    "    else:\n",
    "        text = text.lower()\n",
    "    toks = _token_re.findall(text)\n",
    "    if REMOVE_STOPWORDS:\n",
    "        toks = [t for t in toks if t not in stop_words]\n",
    "    return toks\n",
    "\n",
    "def stem_text(text: str) -> str:\n",
    "    toks = basic_tokenize(text)\n",
    "    toks = [stemmer.stem(t) for t in toks]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "def lemma_text(text: str) -> str:\n",
    "    toks = basic_tokenize(text)\n",
    "    toks = [lemmatizer.lemmatize(t) for t in toks]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Choose one:\n",
    "PREPROC_MODE = \"raw\"  # \"raw\" | \"stem\" | \"lemma\"\n",
    "\n",
    "def preprocess_array(arr):\n",
    "    if PREPROC_MODE == \"raw\":\n",
    "        if CLEAN_TEXT or REMOVE_STOPWORDS:\n",
    "            return np.array([\" \".join(basic_tokenize(x)) for x in tqdm(arr, desc=f\"preprocess={PREPROC_MODE}\")], dtype=object)\n",
    "        return arr\n",
    "    fn = stem_text if PREPROC_MODE == \"stem\" else lemma_text\n",
    "    return np.array([fn(x) for x in tqdm(arr, desc=f\"preprocess={PREPROC_MODE}\")], dtype=object)\n",
    "X_train_p = preprocess_array(X_train)\n",
    "X_val_p   = preprocess_array(X_val)\n",
    "X_test_p  = preprocess_array(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431d31e",
   "metadata": {},
   "source": [
    "## Common evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "012d9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary(y_true, y_pred, name=\"model\", show_report=False):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    print(f\"{name}: acc={acc:.4f} precision={p:.4f} recall={r:.4f} f1={f1:.4f}\")\n",
    "    if show_report:\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        print(\"Classification report:\")\n",
    "        print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "def find_best_threshold(y_true, y_prob, min_t=0.05, max_t=0.95, steps=19):\n",
    "    thresholds = np.linspace(min_t, max_t, steps)\n",
    "    best_t = 0.5\n",
    "    best_f1 = -1.0\n",
    "    for t in thresholds:\n",
    "        f1 = f1_score(y_true, (y_prob >= t).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_t = t\n",
    "    return best_t, best_f1\n",
    "def eval_with_threshold(name, y_val, p_val, y_test, p_test, show_report=False):\n",
    "    best_t, best_f1 = find_best_threshold(y_val, p_val)\n",
    "    print(f\"{name} best threshold (val) = {best_t:.2f}, f1={best_f1:.4f}\")\n",
    "\n",
    "    val_pred = (p_val >= best_t).astype(int)\n",
    "    test_pred = (p_test >= best_t).astype(int)\n",
    "\n",
    "    metrics_val = eval_binary(y_val, val_pred, f\"{name} (val @t={best_t:.2f})\")\n",
    "    metrics_test = eval_binary(y_test, test_pred, f\"{name} (test @t={best_t:.2f})\", show_report=show_report)\n",
    "    return best_t, metrics_val, metrics_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acd61982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words (overall):\n",
      "      term  count\n",
      "0   people  27628\n",
      "1    would  27478\n",
      "2      one  24224\n",
      "3     like  23103\n",
      "4    trump  21687\n",
      "5      get  17733\n",
      "6       us  15449\n",
      "7     time  14334\n",
      "8    think  13353\n",
      "9     know  12547\n",
      "10    many  12485\n",
      "11    even  12283\n",
      "12   right  12245\n",
      "13    good  12169\n",
      "14     see  11020\n",
      "15   years  10978\n",
      "16    make  10806\n",
      "17    much  10717\n",
      "18    well  10572\n",
      "19     way  10369\n",
      "Top bigrams (overall):\n",
      "                  term  count\n",
      "0            years ago   1575\n",
      "1          health care   1490\n",
      "2         donald trump   1188\n",
      "3        united states   1135\n",
      "4            fake news   1040\n",
      "5          many people    989\n",
      "6          sounds like    968\n",
      "7       climate change    939\n",
      "8      president trump    841\n",
      "9          white house    797\n",
      "10          income tax    781\n",
      "11          looks like    767\n",
      "12          would like    765\n",
      "13         people like    719\n",
      "14         even though    710\n",
      "15            year old    701\n",
      "16       supreme court    699\n",
      "17          right wing    689\n",
      "18           last year    676\n",
      "19  federal government    666\n",
      "Top trigrams (overall):\n",
      "                           term  count\n",
      "0                      ha ha ha    223\n",
      "1                new york times    153\n",
      "2       president united states    139\n",
      "3                would like see    137\n",
      "4        president donald trump    117\n",
      "5            health care system    116\n",
      "6            make america great    105\n",
      "7               could care less    104\n",
      "8                 long time ago    101\n",
      "9         united states america    101\n",
      "10           black lives matter     96\n",
      "11               blah blah blah     92\n",
      "12             state income tax     85\n",
      "13          affordable care act     80\n",
      "14               many years ago     78\n",
      "15                  kim jong un     76\n",
      "16  military industrial complex     74\n",
      "17                  etc etc etc     73\n",
      "18              one way another     72\n",
      "19        small business owners     70\n",
      "Top words (class 0 - non-harmful):\n",
      "      term  count\n",
      "0    would  25725\n",
      "1   people  25056\n",
      "2      one  22453\n",
      "3     like  20761\n",
      "4    trump  18675\n",
      "5      get  16203\n",
      "6       us  14282\n",
      "7     time  13344\n",
      "8    think  12249\n",
      "9     many  11697\n",
      "10    know  11510\n",
      "11    good  11426\n",
      "12    even  11300\n",
      "13   right  11183\n",
      "14   years  10334\n",
      "15     see  10230\n",
      "16    much  10074\n",
      "17    make   9986\n",
      "18    well   9877\n",
      "19     way   9631\n",
      "Top words (class 1 - harmful):\n",
      "      term  count\n",
      "0    trump   3012\n",
      "1   people   2572\n",
      "2     like   2342\n",
      "3      one   1771\n",
      "4    would   1753\n",
      "5      get   1530\n",
      "6   stupid   1315\n",
      "7       us   1167\n",
      "8    think   1104\n",
      "9    white   1072\n",
      "10   right   1062\n",
      "11    know   1037\n",
      "12    time    990\n",
      "13    even    983\n",
      "14      go    847\n",
      "15    make    820\n",
      "16    want    818\n",
      "17     see    790\n",
      "18    many    788\n",
      "19  really    758\n"
     ]
    }
   ],
   "source": [
    "# BOW visualization: top unigrams + bigrams/trigrams, overall and by class (train only)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def top_ngrams(texts, ngram_range=(1, 1), top_k=20):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, min_df=2, max_features=50_000)\n",
    "    X = vec.fit_transform(texts)\n",
    "    counts = np.asarray(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vec.get_feature_names_out())\n",
    "    top_idx = counts.argsort()[::-1][:top_k]\n",
    "    return pd.DataFrame({\"term\": vocab[top_idx], \"count\": counts[top_idx]})\n",
    "\n",
    "# Overall most frequent words\n",
    "top_words = top_ngrams(X_train_p, ngram_range=(1, 1), top_k=20)\n",
    "print(\"Top words (overall):\")\n",
    "print(top_words)\n",
    "\n",
    "# Most frequent bigrams and trigrams\n",
    "top_bigrams = top_ngrams(X_train_p, ngram_range=(2, 2), top_k=20)\n",
    "print(\"Top bigrams (overall):\")\n",
    "print(top_bigrams)\n",
    "\n",
    "top_trigrams = top_ngrams(X_train_p, ngram_range=(3, 3), top_k=20)\n",
    "print(\"Top trigrams (overall):\")\n",
    "print(top_trigrams)\n",
    "\n",
    "# Most frequent words by class\n",
    "X_train_p_0 = X_train_p[y_train == 0]\n",
    "X_train_p_1 = X_train_p[y_train == 1]\n",
    "\n",
    "top_words_0 = top_ngrams(X_train_p_0, ngram_range=(1, 1), top_k=20)\n",
    "top_words_1 = top_ngrams(X_train_p_1, ngram_range=(1, 1), top_k=20)\n",
    "\n",
    "print(\"Top words (class 0 - non-harmful):\")\n",
    "print(top_words_0)\n",
    "\n",
    "print(\"Top words (class 1 - harmful):\")\n",
    "print(top_words_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5c505c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "len_train: 100%|██████████| 162000/162000 [00:01<00:00, 108087.38it/s]\n",
      "len_val: 100%|██████████| 18000/18000 [00:00<00:00, 111620.73it/s]\n",
      "len_test: 100%|██████████| 20000/20000 [00:00<00:00, 106457.25it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length quantiles (train): {0.5: 18, 0.75: 37, 0.9: 66, 0.95: 82, 0.98: 90, 0.99: 93}\n",
      "Cutoff: remove all samples with length > 82 tokens (q=0.95)\n",
      "Train: kept=154426, removed=7574, removed%=4.68%\n",
      "Train: min=0, max=148, mean=26.38\n",
      "Val: kept=17117, removed=883, removed%=4.91%\n",
      "Val: min=0, max=113, mean=26.30\n",
      "Test: kept=19029, removed=971, removed%=4.86%\n",
      "Test: min=0, max=148, mean=26.50\n",
      "Top 3 longest train samples (lengths): [148 126 126]\n",
      "After filtering: 154426 17117 19029\n",
      "Pos rate train/val/test: 0.08102910131713571 0.08079686861015364 0.08045614588260024\n"
     ]
    }
   ],
   "source": [
    "# Text length analysis + windowing (train-driven)\n",
    "def get_lengths(texts, desc):\n",
    "    return np.array([len(basic_tokenize(t)) for t in tqdm(texts, desc=desc)], dtype=int)\n",
    "\n",
    "train_lens = get_lengths(X_train_p, \"len_train\")\n",
    "val_lens   = get_lengths(X_val_p, \"len_val\")\n",
    "test_lens  = get_lengths(X_test_p, \"len_test\")\n",
    "\n",
    "# Histogram (train only)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(train_lens, bins=60)\n",
    "plt.title(\"Train text length (tokens)\")\n",
    "plt.xlabel(\"tokens\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "# Quantiles + removal summary (train-driven)\n",
    "qs = [0.5, 0.75, 0.9, 0.95, 0.98, 0.99]\n",
    "q_vals = {q: int(np.quantile(train_lens, q)) for q in qs}\n",
    "print(\"Length quantiles (train):\", q_vals)\n",
    "\n",
    "q = 0.95  # choose cutoff here\n",
    "max_len = q_vals[q]\n",
    "print(f\"Cutoff: remove all samples with length > {max_len} tokens (q={q})\")\n",
    "\n",
    "def removal_stats(lens, max_len, name):\n",
    "    removed = (lens > max_len).sum()\n",
    "    kept = (lens <= max_len).sum()\n",
    "    print(f\"{name}: kept={kept}, removed={removed}, removed%={(removed/len(lens))*100:.2f}%\")\n",
    "    print(f\"{name}: min={lens.min()}, max={lens.max()}, mean={lens.mean():.2f}\")\n",
    "\n",
    "removal_stats(train_lens, max_len, \"Train\")\n",
    "removal_stats(val_lens,   max_len, \"Val\")\n",
    "removal_stats(test_lens,  max_len, \"Test\")\n",
    "\n",
    "# Optional: show a few longest samples being removed (train)\n",
    "idx_sorted = np.argsort(train_lens)[::-1]\n",
    "print(\"Top 3 longest train samples (lengths):\", train_lens[idx_sorted[:3]])\n",
    "\n",
    "# Filter splits\n",
    "def filter_by_len(X, y, lens, max_len):\n",
    "    mask = lens <= max_len\n",
    "    return X[mask], y[mask], lens[mask]\n",
    "\n",
    "X_train_p, y_train, train_lens = filter_by_len(X_train_p, y_train, train_lens, max_len)\n",
    "X_val_p, y_val, val_lens       = filter_by_len(X_val_p, y_val, val_lens, max_len)\n",
    "X_test_p, y_test, test_lens    = filter_by_len(X_test_p, y_test, test_lens, max_len)\n",
    "\n",
    "print(\"After filtering:\", len(X_train_p), len(X_val_p), len(X_test_p))\n",
    "print(\"Pos rate train/val/test:\", y_train.mean(), y_val.mean(), y_test.mean())\n",
    "\n",
    "# Update sequence padding length\n",
    "cfg.seq_max_len = min(cfg.seq_max_len, max_len)\n",
    "\n",
    "# Recompute sampler weights (y_train changed)\n",
    "train_class_counts = np.bincount(y_train)\n",
    "train_class_weights = 1.0 / train_class_counts\n",
    "train_sample_weights = train_class_weights[y_train]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657ade4",
   "metadata": {},
   "source": [
    "## Lexical Analysis\n",
    "\n",
    "An analysis of the most frequent unigrams, bigrams, and trigrams in the training data shows that the dataset is dominated by conversational and opinion-oriented language. Common words such as *people*, *would*, *like*, and *think* indicate informal discussion, while frequent political references (e.g., *trump*, *united states*, *white house*) suggest that political discourse is a central theme.\n",
    "\n",
    "Class-specific analysis reveals a clear lexical distinction between non-harmful and harmful comments. Non-harmful comments primarily contain neutral or descriptive language, whereas harmful comments include explicit offensive terms (e.g., *stupid*) and more emotionally charged expressions. \n",
    "\n",
    "These findings indicate that toxicity in the dataset is often expressed through short, explicit lexical cues rather than long-range contextual dependencies. This explains the strong performance of bag-of-words and TF-IDF based linear models, as they effectively capture the key discriminative signals present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30f177",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization (Optuna)\n",
    "\n",
    "### Optuna: TF-IDF + LogReg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9e53b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-17 17:30:53,663] A new study created in RDB with name: tfidf_logreg_v2\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:30:56,554] Trial 0 finished with value: 0.37181733915582554 and parameters: {'max_features': 40000, 'ngram_range': '1_1', 'min_df': 3, 'C': 0.18410729205738682}. Best is trial 0 with value: 0.37181733915582554.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:08,615] Trial 1 finished with value: 0.277649651701102 and parameters: {'max_features': 30000, 'ngram_range': '1_2', 'min_df': 4, 'C': 1.5958573588141267}. Best is trial 1 with value: 0.277649651701102.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:11,607] Trial 2 finished with value: 0.36434419358329423 and parameters: {'max_features': 20000, 'ngram_range': '1_1', 'min_df': 2, 'C': 0.20366442026830908}. Best is trial 1 with value: 0.277649651701102.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:23,171] Trial 3 finished with value: 0.34468902621823183 and parameters: {'max_features': 30000, 'ngram_range': '1_2', 'min_df': 3, 'C': 0.31245650712608714}. Best is trial 1 with value: 0.277649651701102.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:34,816] Trial 4 finished with value: 0.310933596102476 and parameters: {'max_features': 60000, 'ngram_range': '1_2', 'min_df': 2, 'C': 0.5954553793888989}. Best is trial 1 with value: 0.277649651701102.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:46,512] Trial 5 finished with value: 0.41598619295183403 and parameters: {'max_features': 70000, 'ngram_range': '1_2', 'min_df': 3, 'C': 0.11992724522955164}. Best is trial 1 with value: 0.277649651701102.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:50,236] Trial 6 finished with value: 0.2584844197675555 and parameters: {'max_features': 60000, 'ngram_range': '1_1', 'min_df': 5, 'C': 4.370990468130501}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:53,313] Trial 7 finished with value: 0.3103042911875548 and parameters: {'max_features': 70000, 'ngram_range': '1_1', 'min_df': 4, 'C': 0.5595074635794796}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:31:56,226] Trial 8 finished with value: 0.3469390875700774 and parameters: {'max_features': 20000, 'ngram_range': '1_1', 'min_df': 5, 'C': 0.27520696850790527}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:08,106] Trial 9 finished with value: 0.37363099520234666 and parameters: {'max_features': 60000, 'ngram_range': '1_2', 'min_df': 3, 'C': 0.20609249413202357}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:11,985] Trial 10 finished with value: 0.25991149049403145 and parameters: {'max_features': 80000, 'ngram_range': '1_1', 'min_df': 5, 'C': 4.194704109038071}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:15,631] Trial 11 finished with value: 0.2590655161216623 and parameters: {'max_features': 80000, 'ngram_range': '1_1', 'min_df': 5, 'C': 4.741448176535657}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:19,334] Trial 12 finished with value: 0.2586084987184211 and parameters: {'max_features': 80000, 'ngram_range': '1_1', 'min_df': 5, 'C': 4.422915411165476}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:22,719] Trial 13 finished with value: 0.26472539383896787 and parameters: {'max_features': 50000, 'ngram_range': '1_1', 'min_df': 1, 'C': 2.1198402317468608}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:26,023] Trial 14 finished with value: 0.27044593300687647 and parameters: {'max_features': 60000, 'ngram_range': '1_1', 'min_df': 4, 'C': 1.9177212559869252}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:29,907] Trial 15 finished with value: 0.2619988500547197 and parameters: {'max_features': 70000, 'ngram_range': '1_1', 'min_df': 5, 'C': 3.028289363799703}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:33,124] Trial 16 finished with value: 0.28086120019245475 and parameters: {'max_features': 50000, 'ngram_range': '1_1', 'min_df': 4, 'C': 1.2459097246824833}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:36,577] Trial 17 finished with value: 0.2632678831092603 and parameters: {'max_features': 80000, 'ngram_range': '1_1', 'min_df': 5, 'C': 2.9204643805089106}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:39,681] Trial 18 finished with value: 0.2911923198275004 and parameters: {'max_features': 70000, 'ngram_range': '1_1', 'min_df': 4, 'C': 0.9315731533481453}. Best is trial 6 with value: 0.2584844197675555.\n",
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "[I 2026-01-17 17:32:43,154] Trial 19 finished with value: 0.26314872900194397 and parameters: {'max_features': 40000, 'ngram_range': '1_1', 'min_df': 5, 'C': 3.122034348938151}. Best is trial 6 with value: 0.2584844197675555.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF params: {'max_features': 60000, 'ngram_range': (1, 1), 'min_df': 5}\n",
      "LogReg params: {'C': 4.370990468130501, 'max_iter': 300}\n",
      "Optuna DB: c:\\Stjepan\\Fax\\NepredniML\\Projekt\\optuna_tfidf_logreg.db\n"
     ]
    }
   ],
   "source": [
    "best_tfidf_params = {\n",
    "    \"max_features\": cfg.max_features,\n",
    "    \"ngram_range\": cfg.ngram_range,\n",
    "    \"min_df\": cfg.min_df,\n",
    "}\n",
    "best_logreg_params = {\n",
    "    \"C\": 1.0,\n",
    "    \"max_iter\": 300,\n",
    "}\n",
    "\n",
    "N_TRIALS_TFIDF = 20\n",
    "\n",
    "def objective_tfidf_logreg(trial):\n",
    "    max_features = trial.suggest_int(\"max_features\", 20000, 80000, step=10000)\n",
    "    ngram_key = trial.suggest_categorical(\"ngram_range\", [\"1_1\", \"1_2\"])\n",
    "    ngram_range = (1, 1) if ngram_key == \"1_1\" else (1, 2)\n",
    "    min_df = trial.suggest_int(\"min_df\", 1, 5)\n",
    "    C = trial.suggest_float(\"C\", 0.1, 5.0, log=True)\n",
    "\n",
    "    tfidf_opt = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        min_df=min_df\n",
    "    )\n",
    "    Xtr = tfidf_opt.fit_transform(X_train_p)\n",
    "    Xva = tfidf_opt.transform(X_val_p)\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        max_iter=300,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced\",\n",
    "        C=C\n",
    "    )\n",
    "    model.fit(Xtr, y_train)\n",
    "\n",
    "    val_prob = model.predict_proba(Xva)[:, 1]\n",
    "    return log_loss(y_val, val_prob, labels=[0, 1])\n",
    "\n",
    "optuna_db_path_tfidf = os.path.abspath(\"optuna_tfidf_logreg.db\")\n",
    "study_tfidf = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    storage=f\"sqlite:///{optuna_db_path_tfidf}\",\n",
    "    study_name=\"tfidf_logreg_v2\",\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(seed=cfg.seed)\n",
    ")\n",
    "\n",
    "study_tfidf.optimize(objective_tfidf_logreg, n_trials=N_TRIALS_TFIDF)\n",
    "\n",
    "best_ngram_val = study_tfidf.best_params[\"ngram_range\"]\n",
    "if isinstance(best_ngram_val, str):\n",
    "    best_ngram = (1, 1) if best_ngram_val == \"1_1\" else (1, 2)\n",
    "elif isinstance(best_ngram_val, list):\n",
    "    best_ngram = tuple(best_ngram_val)\n",
    "else:\n",
    "    best_ngram = best_ngram_val\n",
    "\n",
    "best_tfidf_params = {\n",
    "    \"max_features\": study_tfidf.best_params[\"max_features\"],\n",
    "    \"ngram_range\": best_ngram,\n",
    "    \"min_df\": study_tfidf.best_params[\"min_df\"],\n",
    "}\n",
    "best_logreg_params = {\n",
    "    \"C\": study_tfidf.best_params[\"C\"],\n",
    "    \"max_iter\": 300,\n",
    "}\n",
    "\n",
    "print(\"TF-IDF params:\", best_tfidf_params)\n",
    "print(\"LogReg params:\", best_logreg_params)\n",
    "print(\"Optuna DB:\", optuna_db_path_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f264fb2",
   "metadata": {},
   "source": [
    "## Bag‑of‑Words + LogReg (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "814e765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW+LogReg best threshold (val) = 0.60, f1=0.5791\n",
      "BOW+LogReg (val @t=0.60): acc=0.9268 precision=0.5408 recall=0.6233 f1=0.5791\n",
      "BOW+LogReg (test @t=0.60): acc=0.9277 precision=0.5419 recall=0.6545 f1=0.5929\n"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(\n",
    "    max_features=cfg.max_features,\n",
    "    ngram_range=cfg.ngram_range,\n",
    "    min_df=cfg.min_df\n",
    ")\n",
    "\n",
    "Xtr_bow = bow.fit_transform(X_train_p)\n",
    "Xva_bow = bow.transform(X_val_p)\n",
    "Xte_bow = bow.transform(X_test_p)\n",
    "\n",
    "logreg_bow = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "logreg_bow.fit(Xtr_bow, y_train)\n",
    "\n",
    "val_prob = logreg_bow.predict_proba(Xva_bow)[:, 1]\n",
    "test_prob = logreg_bow.predict_proba(Xte_bow)[:, 1]\n",
    "\n",
    "bow_t, metrics_bow_val, metrics_bow_test = eval_with_threshold(\n",
    "    \"BOW+LogReg\", y_val, val_prob, y_test, test_prob\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c06e1",
   "metadata": {},
   "source": [
    "## TF-IDF + Logistic Regression (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd5e3f",
   "metadata": {},
   "source": [
    "### Vectorize TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a83698c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154426, 28272), (17117, 28272))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    max_features=best_tfidf_params[\"max_features\"],\n",
    "    ngram_range=best_tfidf_params[\"ngram_range\"],\n",
    "    min_df=best_tfidf_params[\"min_df\"]\n",
    ")\n",
    "\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train_p)\n",
    "Xva_tfidf = tfidf.transform(X_val_p)\n",
    "Xte_tfidf = tfidf.transform(X_test_p)\n",
    "\n",
    "Xtr_tfidf.shape, Xva_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccc9cb",
   "metadata": {},
   "source": [
    "### Logistic Regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "826dce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stjepan.vinski\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF+LogReg best threshold (val) = 0.80, f1=0.5892\n",
      "TFIDF+LogReg (val @t=0.80): acc=0.9380 precision=0.6342 recall=0.5503 f1=0.5892\n",
      "TFIDF+LogReg (test @t=0.80): acc=0.9404 precision=0.6422 recall=0.5839 f1=0.6117\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(\n",
    "    max_iter=best_logreg_params[\"max_iter\"],\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",  # useful for skew\n",
    "    C=best_logreg_params[\"C\"]\n",
    ")\n",
    "\n",
    "logreg.fit(Xtr_tfidf, y_train)\n",
    "\n",
    "val_prob = logreg.predict_proba(Xva_tfidf)[:, 1]\n",
    "test_prob = logreg.predict_proba(Xte_tfidf)[:, 1]\n",
    "\n",
    "tfidf_t, metrics_logreg_val, metrics_logreg_test = eval_with_threshold(\n",
    "    \"TFIDF+LogReg\", y_val, val_prob, y_test, test_prob\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243f1750",
   "metadata": {},
   "source": [
    "## TF-IDF + MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca38331",
   "metadata": {},
   "source": [
    "### Sparse TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a22e9294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154426, 28272)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SparseTfidfDataset(Dataset):\n",
    "    def __init__(self, X_csr, y):\n",
    "        self.X = X_csr.tocsr()\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X[idx]\n",
    "        x = torch.from_numpy(row.toarray().ravel()).float()\n",
    "        y = torch.tensor(self.y[idx]).float()\n",
    "        return x, y\n",
    "\n",
    "train_ds = SparseTfidfDataset(Xtr_tfidf, y_train)\n",
    "val_ds   = SparseTfidfDataset(Xva_tfidf, y_val)\n",
    "test_ds  = SparseTfidfDataset(Xte_tfidf, y_test)\n",
    "\n",
    "sampler = WeightedRandomSampler(train_sample_weights, num_samples=len(train_sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, sampler=sampler)\n",
    "val_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "Xtr_tfidf.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e6d37",
   "metadata": {},
   "source": [
    "### MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01fc22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=256, dropout=0.3, activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.activation = activation.lower()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden, 1)\n",
    "\n",
    "        # weight init\n",
    "        if self.activation == \"relu\":\n",
    "            nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.fc1(x))\n",
    "        if self.activation == \"relu\":\n",
    "            x = F.relu(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            x = torch.tanh(x)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            x = torch.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(\"activation must be relu/tanh/sigmoid\")\n",
    "\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x).squeeze(1)  # logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28314f3",
   "metadata": {},
   "source": [
    "### Optuna: TF-IDF + MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b4b06e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-17 17:33:00,872] A new study created in RDB with name: tfidf_mlp\n",
      "[I 2026-01-17 17:33:36,281] Trial 0 finished with value: 0.21535084663292944 and parameters: {'hidden': 128, 'dropout': 0.2993292420985183, 'lr': 0.00018410729205738696, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:34:14,468] Trial 1 finished with value: 0.22167825670733857 and parameters: {'hidden': 128, 'dropout': 0.48495492608099716, 'lr': 0.002595942550311264, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:34:49,737] Trial 2 finished with value: 0.26390009573695883 and parameters: {'hidden': 128, 'dropout': 0.14561457009902096, 'lr': 0.001095266274863256, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:35:24,705] Trial 3 finished with value: 0.2630160161147519 and parameters: {'hidden': 128, 'dropout': 0.2571172192068058, 'lr': 0.0010150667045928574, 'batch_size': 256}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:36:00,526] Trial 4 finished with value: 0.250587665633202 and parameters: {'hidden': 256, 'dropout': 0.40419867405823057, 'lr': 0.00032925293631105276, 'batch_size': 256}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:36:41,104] Trial 5 finished with value: 0.24561647039996956 and parameters: {'hidden': 128, 'dropout': 0.45466020103939103, 'lr': 0.00027520696850790545, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:37:16,638] Trial 6 finished with value: 0.22839082758435164 and parameters: {'hidden': 256, 'dropout': 0.3875664116805573, 'lr': 0.003946212980759096, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:37:54,060] Trial 7 finished with value: 0.2658243938281992 and parameters: {'hidden': 128, 'dropout': 0.16266516538163217, 'lr': 0.00045745782054754043, 'batch_size': 256}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:38:34,771] Trial 8 finished with value: 0.23474109235500462 and parameters: {'hidden': 128, 'dropout': 0.40109849037701983, 'lr': 0.00013386261584543918, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:39:13,451] Trial 9 finished with value: 0.23526287903614104 and parameters: {'hidden': 128, 'dropout': 0.36450358402049365, 'lr': 0.0020434554984161395, 'batch_size': 256}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:39:49,419] Trial 10 finished with value: 0.24923143532326714 and parameters: {'hidden': 64, 'dropout': 0.0029985914779086797, 'lr': 0.00010299654796681868, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:40:30,394] Trial 11 finished with value: 0.2516197064055623 and parameters: {'hidden': 64, 'dropout': 0.2834632799309926, 'lr': 0.004554515335693162, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:41:06,110] Trial 12 finished with value: 0.24505033639711832 and parameters: {'hidden': 128, 'dropout': 0.4982672051714142, 'lr': 0.001957937345337645, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:41:46,958] Trial 13 finished with value: 0.2508334830701926 and parameters: {'hidden': 128, 'dropout': 0.31183213561285816, 'lr': 0.0001955263401110267, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:42:22,759] Trial 14 finished with value: 0.2640581254355434 and parameters: {'hidden': 256, 'dropout': 0.17879979361685228, 'lr': 0.0005924321031362263, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:43:03,905] Trial 15 finished with value: 0.2690970575659048 and parameters: {'hidden': 64, 'dropout': 0.0769923672433731, 'lr': 0.002101280432296517, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:43:45,200] Trial 16 finished with value: 0.24724676190984995 and parameters: {'hidden': 128, 'dropout': 0.3280669206817701, 'lr': 0.001066247683417251, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:44:20,844] Trial 17 finished with value: 0.25072560665477495 and parameters: {'hidden': 128, 'dropout': 0.20954155236345323, 'lr': 0.002933957983740699, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:45:01,861] Trial 18 finished with value: 0.23442918034178453 and parameters: {'hidden': 64, 'dropout': 0.4960981647510031, 'lr': 0.0002027372131533072, 'batch_size': 128}. Best is trial 0 with value: 0.21535084663292944.\n",
      "[I 2026-01-17 17:45:37,792] Trial 19 finished with value: 0.24928156580157235 and parameters: {'hidden': 256, 'dropout': 0.4417294913597396, 'lr': 0.000716179623260512, 'batch_size': 512}. Best is trial 0 with value: 0.21535084663292944.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP params: {'hidden': 128, 'dropout': 0.2993292420985183, 'lr': 0.00018410729205738696, 'batch_size': 512}\n",
      "Optuna DB: c:\\Stjepan\\Fax\\NepredniML\\Projekt\\optuna_tfidf_mlp.db\n"
     ]
    }
   ],
   "source": [
    "best_mlp_params = {\n",
    "    \"hidden\": 256,\n",
    "    \"dropout\": 0.3,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": cfg.batch_size,\n",
    "}\n",
    "\n",
    "N_TRIALS_MLP = 20\n",
    "EPOCHS_MLP_OPTUNA = 2\n",
    "\n",
    "def objective_mlp(trial):\n",
    "    hidden = trial.suggest_categorical(\"hidden\", [64, 128, 256])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n",
    "\n",
    "    train_ds_opt = SparseTfidfDataset(Xtr_tfidf, y_train)\n",
    "    val_ds_opt = SparseTfidfDataset(Xva_tfidf, y_val)\n",
    "\n",
    "    sampler = WeightedRandomSampler(train_sample_weights, num_samples=len(train_sample_weights), replacement=True)\n",
    "    train_loader_opt = DataLoader(train_ds_opt, batch_size=batch_size, sampler=sampler)\n",
    "    val_loader_opt = DataLoader(val_ds_opt, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TfidfMLP(in_dim=Xtr_tfidf.shape[1], hidden=hidden, dropout=dropout, activation=\"relu\").to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    for _ in range(EPOCHS_MLP_OPTUNA):\n",
    "        model.train()\n",
    "        for x, y in train_loader_opt:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "            opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader_opt:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n += bs\n",
    "    return total_loss / n\n",
    "\n",
    "optuna_db_path_mlp = os.path.abspath(\"optuna_tfidf_mlp.db\")\n",
    "study_mlp = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    storage=f\"sqlite:///{optuna_db_path_mlp}\",\n",
    "    study_name=\"tfidf_mlp\",\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(seed=cfg.seed)\n",
    ")\n",
    "\n",
    "study_mlp.optimize(objective_mlp, n_trials=N_TRIALS_MLP)\n",
    "\n",
    "best_mlp_params = {\n",
    "    \"hidden\": study_mlp.best_params[\"hidden\"],\n",
    "    \"dropout\": study_mlp.best_params[\"dropout\"],\n",
    "    \"lr\": study_mlp.best_params[\"lr\"],\n",
    "    \"batch_size\": study_mlp.best_params[\"batch_size\"],\n",
    "}\n",
    "\n",
    "print(\"MLP params:\", best_mlp_params)\n",
    "print(\"Optuna DB:\", optuna_db_path_mlp)\n",
    "\n",
    "# Rebuild loaders with best batch size\n",
    "mlp_batch_size = best_mlp_params[\"batch_size\"]\n",
    "sampler = WeightedRandomSampler(train_sample_weights, num_samples=len(train_sample_weights), replacement=True)\n",
    "train_loader = DataLoader(train_ds, batch_size=mlp_batch_size, sampler=sampler)\n",
    "val_loader = DataLoader(val_ds, batch_size=mlp_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=mlp_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99513b68",
   "metadata": {},
   "source": [
    "### Train/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23ad32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model):\n",
    "    if cfg.optimizer.lower() == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    if cfg.optimizer.lower() == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)\n",
    "    raise ValueError(\"optimizer must be adam/sgd\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true.astype(int), y_prob, y_pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader_probs_and_loss(model, loader, criterion):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.cpu().numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys).astype(int)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    return y_true, y_prob, total_loss / n\n",
    "\n",
    "def fit_binary(model, train_loader, val_loader, epochs=3):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    opt = make_optimizer(model)\n",
    "\n",
    "    best_ap = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        yv, pv, val_loss = eval_loader_probs_and_loss(model, val_loader, criterion)\n",
    "        val_ap = average_precision_score(yv, pv)\n",
    "        print(f\"Epoch {ep}/{epochs} train_loss={total_loss/n:.4f} val_loss={val_loss:.4f} val_ap={val_ap:.4f}\")\n",
    "\n",
    "        if val_ap > best_ap:\n",
    "            best_ap = val_ap\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43928f",
   "metadata": {},
   "source": [
    "### Train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d822cd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 train_loss=0.2891 val_loss=0.2251 val_ap=0.5871\n",
      "Epoch 2/3 train_loss=0.0873 val_loss=0.2170 val_ap=0.5914\n",
      "Epoch 3/3 train_loss=0.0470 val_loss=0.2303 val_ap=0.5821\n",
      "TFIDF+MLP best threshold (val) = 0.65, f1=0.5568\n",
      "TFIDF+MLP (val @t=0.65): acc=0.9337 precision=0.6053 recall=0.5155 f1=0.5568\n",
      "TFIDF+MLP (test @t=0.65): acc=0.9368 precision=0.6228 recall=0.5434 f1=0.5804\n"
     ]
    }
   ],
   "source": [
    "cfg.optimizer = \"adam\"\n",
    "cfg.lr = best_mlp_params[\"lr\"]\n",
    "cfg.epochs = 3\n",
    "\n",
    "mlp = TfidfMLP(in_dim=Xtr_tfidf.shape[1], hidden=best_mlp_params[\"hidden\"], dropout=best_mlp_params[\"dropout\"], activation=\"relu\")\n",
    "mlp = fit_binary(mlp, train_loader, val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yv, pv, _ = eval_loader(mlp, val_loader)\n",
    "yt, pt, _ = eval_loader(mlp, test_loader)\n",
    "\n",
    "mlp_t, _, metrics_mlp_test = eval_with_threshold(\n",
    "    \"TFIDF+MLP\", yv, pv, yt, pt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ddf157",
   "metadata": {},
   "source": [
    "## LSTM and BiLSTM+Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a8d7d",
   "metadata": {},
   "source": [
    "### Build vocabulary + encode sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de876f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build_vocab: 100%|██████████| 154426/154426 [00:01<00:00, 85605.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD = 0\n",
    "UNK = 1\n",
    "\n",
    "def build_vocab(texts, vocab_size=50_000):\n",
    "    counter = Counter()\n",
    "    for t in tqdm(texts, desc=\"build_vocab\"):\n",
    "        counter.update(basic_tokenize(t))\n",
    "    most = counter.most_common(vocab_size - 2)\n",
    "    stoi = {\"<PAD>\": PAD, \"<UNK>\": UNK}\n",
    "    for i, (w, _) in enumerate(most, start=2):\n",
    "        stoi[w] = i\n",
    "    return stoi\n",
    "\n",
    "stoi = build_vocab(X_train_p, vocab_size=cfg.vocab_size)\n",
    "itos_size = len(stoi)\n",
    "itos_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b2c5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, stoi, max_len=200):\n",
    "    toks = basic_tokenize(text)\n",
    "    ids = [stoi.get(w, UNK) for w in toks[:max_len]]\n",
    "    if len(ids) == 0:\n",
    "        ids = [UNK]\n",
    "    return ids\n",
    "\n",
    "\n",
    "def pad_batch(batch_ids, pad_id=PAD):\n",
    "    lens = torch.tensor([max(1, len(x)) for x in batch_ids], dtype=torch.long)\n",
    "    maxlen = int(lens.max().item())\n",
    "    padded = torch.full((len(batch_ids), maxlen), pad_id, dtype=torch.long)\n",
    "\n",
    "    for i, ids in enumerate(batch_ids):\n",
    "        if len(ids) == 0:\n",
    "            ids = [UNK]\n",
    "        padded[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    return padded, lens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d2db1",
   "metadata": {},
   "source": [
    "### Sequence Dataset + collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6258a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, texts, labels, stoi, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype(np.float32)\n",
    "        self.stoi = stoi\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = encode_text(self.texts[idx], self.stoi, self.max_len)\n",
    "        y = self.labels[idx]\n",
    "        return ids, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ids_list, ys = zip(*batch)\n",
    "    x, lens = pad_batch(ids_list, pad_id=PAD)\n",
    "    y = torch.tensor(ys, dtype=torch.float32)\n",
    "    return x, lens, y\n",
    "\n",
    "seq_train = SeqDataset(X_train_p, y_train, stoi, cfg.seq_max_len)\n",
    "seq_val   = SeqDataset(X_val_p, y_val, stoi, cfg.seq_max_len)\n",
    "seq_test  = SeqDataset(X_test_p, y_test, stoi, cfg.seq_max_len)\n",
    "\n",
    "seq_sampler = WeightedRandomSampler(train_sample_weights, num_samples=len(train_sample_weights), replacement=True)\n",
    "\n",
    "seq_train_loader = DataLoader(seq_train, batch_size=cfg.batch_size, sampler=seq_sampler, collate_fn=collate_fn)\n",
    "seq_val_loader   = DataLoader(seq_val, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "seq_test_loader  = DataLoader(seq_test, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ad659",
   "metadata": {},
   "source": [
    "### LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd649652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, layers=1, dropout=0.3, bidir=False):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden, num_layers=layers,\n",
    "            batch_first=True, dropout=dropout if layers > 1 else 0.0,\n",
    "            bidirectional=bidir\n",
    "        )\n",
    "        out_dim = hidden * (2 if bidir else 1)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_dim, 1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        emb = self.emb(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed)\n",
    "\n",
    "        # last layer hidden\n",
    "        if self.lstm.bidirectional:\n",
    "            h_last = torch.cat([h[-2], h[-1]], dim=1)\n",
    "        else:\n",
    "            h_last = h[-1]\n",
    "\n",
    "        h_last = self.drop(h_last)\n",
    "        logits = self.fc(h_last).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692120a",
   "metadata": {},
   "source": [
    "### Train/eval loops for sequence models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "304526b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_seq(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for x, lens, y in loader:\n",
    "        x, lens = x.to(device), lens.to(device)\n",
    "        logits = model(x, lens)\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys).astype(int)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    return y_true, y_prob, y_pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_seq_probs_and_loss(model, loader, criterion):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "    for x, lens, y in loader:\n",
    "        x, lens, y = x.to(device), lens.to(device), y.to(device)\n",
    "        logits = model(x, lens)\n",
    "        loss = criterion(logits, y)\n",
    "        bs = x.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        n += bs\n",
    "        prob = torch.sigmoid(logits).cpu().numpy()\n",
    "        ys.append(y.cpu().numpy())\n",
    "        ps.append(prob)\n",
    "    y_true = np.concatenate(ys).astype(int)\n",
    "    y_prob = np.concatenate(ps)\n",
    "    return y_true, y_prob, total_loss / n\n",
    "\n",
    "def fit_seq(model, train_loader, val_loader, epochs=3):\n",
    "    model = model.to(device)\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    opt = make_optimizer(model)\n",
    "\n",
    "    best_ap = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        n = 0\n",
    "\n",
    "        for x, lens, y in train_loader:\n",
    "            x, lens, y = x.to(device), lens.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, lens)\n",
    "            loss = crit(logits, y)\n",
    "            loss.backward()\n",
    "\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            n += bs\n",
    "\n",
    "        yv, pv, val_loss = eval_seq_probs_and_loss(model, val_loader, crit)\n",
    "        val_ap = average_precision_score(yv, pv)\n",
    "        print(f\"Epoch {ep}/{epochs} train_loss={total_loss/n:.4f} val_loss={val_loss:.4f} val_ap={val_ap:.4f}\")\n",
    "\n",
    "        if val_ap > best_ap:\n",
    "            best_ap = val_ap\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f30a7be",
   "metadata": {},
   "source": [
    "### Optuna: LSTM/BiLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a68df249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-17 17:46:51,631] A new study created in RDB with name: lstm_optuna\n",
      "[I 2026-01-17 17:47:14,901] Trial 0 finished with value: 0.3293562870044578 and parameters: {'emb_ratio': 0.48727005942368123, 'hidden_ratio': 0.4802857225639665, 'dropout': 0.146398788362281, 'lr': 0.0010401663679887319}. Best is trial 0 with value: 0.3293562870044578.\n",
      "[I 2026-01-17 17:47:37,785] Trial 1 finished with value: 0.2859268188546206 and parameters: {'emb_ratio': 0.37800932022121825, 'hidden_ratio': 0.16239780813448107, 'dropout': 0.011616722433639893, 'lr': 0.0029621516588303515}. Best is trial 1 with value: 0.2859268188546206.\n",
      "[I 2026-01-17 17:48:01,424] Trial 2 finished with value: 0.26472070615912957 and parameters: {'emb_ratio': 0.6005575058716044, 'hidden_ratio': 0.3832290311184182, 'dropout': 0.004116898859160489, 'lr': 0.0044447541666908135}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:48:24,990] Trial 3 finished with value: 0.54887662346817 and parameters: {'emb_ratio': 0.7162213204002108, 'hidden_ratio': 0.18493564427131048, 'dropout': 0.03636499344142013, 'lr': 0.0002049268011541737}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:48:54,252] Trial 4 finished with value: 0.4909681825463151 and parameters: {'emb_ratio': 0.45212112147976885, 'hidden_ratio': 0.3099025726528951, 'dropout': 0.08638900372842316, 'lr': 0.0003124565071260876}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:49:19,334] Trial 5 finished with value: 0.4651220734935501 and parameters: {'emb_ratio': 0.6059264473611897, 'hidden_ratio': 0.15579754426081674, 'dropout': 0.058428929707043636, 'lr': 0.0004192159350410976}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:49:48,204] Trial 6 finished with value: 0.4502993974635108 and parameters: {'emb_ratio': 0.5280349921085179, 'hidden_ratio': 0.41407038455720546, 'dropout': 0.03993475643167195, 'lr': 0.000747599299995651}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:50:12,192] Trial 7 finished with value: 0.6610837947189192 and parameters: {'emb_ratio': 0.5962072844310212, 'hidden_ratio': 0.1185801650879991, 'dropout': 0.12150897038028768, 'lr': 0.00019485671251272575}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:50:38,369] Trial 8 finished with value: 0.34989690063308526 and parameters: {'emb_ratio': 0.33252579649263975, 'hidden_ratio': 0.4795542149013333, 'dropout': 0.19312640661491187, 'lr': 0.0023628864184236428}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:51:04,064] Trial 9 finished with value: 0.43479975917499925 and parameters: {'emb_ratio': 0.45230688458668533, 'hidden_ratio': 0.13906884560255356, 'dropout': 0.1368466053024314, 'lr': 0.0005595074635794797}. Best is trial 2 with value: 0.26472070615912957.\n",
      "[I 2026-01-17 17:51:27,864] Trial 10 finished with value: 0.23891088949252967 and parameters: {'emb_ratio': 0.776994771972554, 'hidden_ratio': 0.3235403593029869, 'dropout': 0.0015094638632586477, 'lr': 0.004194420841895818}. Best is trial 10 with value: 0.23891088949252967.\n",
      "[I 2026-01-17 17:51:52,218] Trial 11 finished with value: 0.23768443216534846 and parameters: {'emb_ratio': 0.7823171596116492, 'hidden_ratio': 0.33041085600236286, 'dropout': 0.0029524390325197433, 'lr': 0.004744945346857696}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:52:16,061] Trial 12 finished with value: 0.26833349397550604 and parameters: {'emb_ratio': 0.7851143034505196, 'hidden_ratio': 0.27225907838625985, 'dropout': 0.07775112562716173, 'lr': 0.0015329433094942114}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:52:49,244] Trial 13 finished with value: 0.2918279802607008 and parameters: {'emb_ratio': 0.7094816769577929, 'hidden_ratio': 0.29104585705925534, 'dropout': 0.006190673608908518, 'lr': 0.004436353679356643}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:53:16,709] Trial 14 finished with value: 0.275372636355118 and parameters: {'emb_ratio': 0.7973355227246846, 'hidden_ratio': 0.3655302357156448, 'dropout': 0.03561951834112205, 'lr': 0.0019262598214642502}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:53:44,833] Trial 15 finished with value: 0.2916760462226172 and parameters: {'emb_ratio': 0.695734666124976, 'hidden_ratio': 0.2300570135729482, 'dropout': 0.05186958929954119, 'lr': 0.0012849486770318643}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:54:14,041] Trial 16 finished with value: 0.26566505141888375 and parameters: {'emb_ratio': 0.65058129769688, 'hidden_ratio': 0.35060236532951866, 'dropout': 0.021466900747858474, 'lr': 0.00492268686882158}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:54:42,544] Trial 17 finished with value: 0.28128186244862086 and parameters: {'emb_ratio': 0.7603517397969456, 'hidden_ratio': 0.42594747654823856, 'dropout': 0.0645866283451903, 'lr': 0.0029884965236927907}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:55:08,672] Trial 18 finished with value: 0.26193809103602217 and parameters: {'emb_ratio': 0.6602285047472973, 'hidden_ratio': 0.23560565778823855, 'dropout': 0.0994170787221705, 'lr': 0.0031202313108807524}. Best is trial 11 with value: 0.23768443216534846.\n",
      "[I 2026-01-17 17:55:32,500] Trial 19 finished with value: 0.32107037730659077 and parameters: {'emb_ratio': 0.742847881114328, 'hidden_ratio': 0.34784836718201456, 'dropout': 0.18272051718809276, 'lr': 0.0008694847100915745}. Best is trial 11 with value: 0.23768443216534846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq params: {'emb_dim': 100, 'hidden': 42, 'dropout': 0.0029524390325197433, 'lr': 0.004744945346857696}\n",
      "Optuna DB: c:\\Stjepan\\Fax\\NepredniML\\Projekt\\optuna_lstm.db\n"
     ]
    }
   ],
   "source": [
    "best_seq_params = {\n",
    "    \"emb_dim\": cfg.emb_dim,\n",
    "    \"hidden\": cfg.lstm_hidden,\n",
    "    \"dropout\": cfg.dropout,\n",
    "    \"lr\": cfg.lr,\n",
    "}\n",
    "\n",
    "N_TRIALS_SEQ = 20\n",
    "EPOCHS_SEQ_OPTUNA = 2\n",
    "\n",
    "def objective_seq(trial):\n",
    "    emb_ratio = trial.suggest_float(\"emb_ratio\", 0.3, 0.8)\n",
    "    hidden_ratio = trial.suggest_float(\"hidden_ratio\", 0.1, 0.5)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.2)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "\n",
    "    emb_dim = max(16, int(cfg.emb_dim * emb_ratio))\n",
    "    hidden = max(8, int(cfg.lstm_hidden * hidden_ratio))\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        vocab_size=len(stoi),\n",
    "        emb_dim=emb_dim,\n",
    "        hidden=hidden,\n",
    "        layers=cfg.lstm_layers,\n",
    "        dropout=dropout,\n",
    "        bidir=False\n",
    "    ).to(device)\n",
    "\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for _ in range(EPOCHS_SEQ_OPTUNA):\n",
    "        model.train()\n",
    "        for x, lens, y in seq_train_loader:\n",
    "            x, lens, y = x.to(device), lens.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, lens)\n",
    "            loss = crit(logits, y)\n",
    "            loss.backward()\n",
    "            if cfg.grad_clip_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip_norm)\n",
    "            opt.step()\n",
    "\n",
    "        _, _, val_loss = eval_seq_probs_and_loss(model, seq_val_loader, crit)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "optuna_db_path_seq = os.path.abspath(\"optuna_lstm.db\")\n",
    "study_seq = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    storage=f\"sqlite:///{optuna_db_path_seq}\",\n",
    "    study_name=\"lstm_optuna\",\n",
    "    load_if_exists=True,\n",
    "    sampler=optuna.samplers.TPESampler(seed=cfg.seed)\n",
    ")\n",
    "\n",
    "study_seq.optimize(objective_seq, n_trials=N_TRIALS_SEQ)\n",
    "\n",
    "best_seq_params = {\n",
    "    \"emb_dim\": max(16, int(cfg.emb_dim * study_seq.best_params[\"emb_ratio\"])),\n",
    "    \"hidden\": max(8, int(cfg.lstm_hidden * study_seq.best_params[\"hidden_ratio\"])),\n",
    "    \"dropout\": study_seq.best_params[\"dropout\"],\n",
    "    \"lr\": study_seq.best_params[\"lr\"],\n",
    "}\n",
    "\n",
    "print(\"Seq params:\", best_seq_params)\n",
    "print(\"Optuna DB:\", optuna_db_path_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe1206",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c8bd78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 train_loss=0.3792 val_loss=0.2595 val_ap=0.6451\n",
      "Epoch 2/3 train_loss=0.2261 val_loss=0.2688 val_ap=0.6352\n",
      "Epoch 3/3 train_loss=0.1805 val_loss=0.2987 val_ap=0.6367\n",
      "LSTM best threshold (val) = 0.85, f1=0.6086\n",
      "LSTM (val @t=0.85): acc=0.9378 precision=0.6188 recall=0.5987 f1=0.6086\n",
      "LSTM (test @t=0.85): acc=0.9388 precision=0.6231 recall=0.6048 f1=0.6139\n"
     ]
    }
   ],
   "source": [
    "cfg.optimizer = \"adam\"\n",
    "cfg.lr = best_seq_params[\"lr\"]\n",
    "cfg.epochs = 3\n",
    "cfg.emb_dim = best_seq_params[\"emb_dim\"]\n",
    "cfg.lstm_hidden = best_seq_params[\"hidden\"]\n",
    "cfg.dropout = best_seq_params[\"dropout\"]\n",
    "cfg.grad_clip_norm = 1.0  # very relevant for RNN/LSTM\n",
    "\n",
    "lstm = LSTMClassifier(vocab_size=len(stoi), emb_dim=cfg.emb_dim, hidden=cfg.lstm_hidden,\n",
    "                      layers=cfg.lstm_layers, dropout=cfg.dropout, bidir=False)\n",
    "lstm = fit_seq(lstm, seq_train_loader, seq_val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yv, pv, _ = eval_seq(lstm, seq_val_loader)\n",
    "yt, pt, _ = eval_seq(lstm, seq_test_loader)\n",
    "\n",
    "lstm_t, _, metrics_lstm_test = eval_with_threshold(\n",
    "    \"LSTM\", yv, pv, yt, pt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a6d53",
   "metadata": {},
   "source": [
    "## BiLSTM + Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb11b0",
   "metadata": {},
   "source": [
    "### Additive attention on token outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbad7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden=128, layers=1, dropout=0.3, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden, num_layers=layers,\n",
    "            batch_first=True, dropout=dropout if layers > 1 else 0.0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Multi-head self-attention on sequence embeddings (Q=K=V)\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=hidden * 2,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden * 2, 1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        emb = self.emb(x)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(emb, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)  # (B,T,H*2)\n",
    "\n",
    "        # mask padding\n",
    "        B, T, _ = out.shape\n",
    "        key_padding_mask = torch.arange(T, device=lens.device).unsqueeze(0) >= lens.unsqueeze(1)  # True for pad\n",
    "\n",
    "        # self-attention (Q=K=V=out), return averaged weights across heads\n",
    "        attn_out, attn_weights = self.mha(\n",
    "            out, out, out,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=True,\n",
    "            average_attn_weights=True\n",
    "        )\n",
    "\n",
    "        # masked mean pool\n",
    "        mask = (~key_padding_mask).unsqueeze(-1)  # True for real tokens\n",
    "        attn_out = attn_out * mask\n",
    "        context = attn_out.sum(dim=1) / lens.unsqueeze(1)\n",
    "\n",
    "        context = self.drop(context)\n",
    "        logits = self.fc(context).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58842f68",
   "metadata": {},
   "source": [
    "### Train attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "894e25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 train_loss=0.3405 val_loss=0.3425 val_ap=0.6178\n",
      "Epoch 2/3 train_loss=0.2100 val_loss=0.2503 val_ap=0.6427\n",
      "Epoch 3/3 train_loss=0.1617 val_loss=0.2554 val_ap=0.6277\n",
      "BiLSTM+Attention best threshold (val) = 0.75, f1=0.6098\n",
      "BiLSTM+Attention (val @t=0.75): acc=0.9297 precision=0.5529 recall=0.6797 f1=0.6098\n",
      "BiLSTM+Attention (test @t=0.75): acc=0.9283 precision=0.5442 recall=0.6669 f1=0.5994\n",
      "Confusion matrix:\n",
      "[[16643   855]\n",
      " [  510  1021]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9703    0.9511    0.9606     17498\n",
      "           1     0.5442    0.6669    0.5994      1531\n",
      "\n",
      "    accuracy                         0.9283     19029\n",
      "   macro avg     0.7573    0.8090    0.7800     19029\n",
      "weighted avg     0.9360    0.9283    0.9315     19029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg.lr = best_seq_params[\"lr\"]\n",
    "cfg.emb_dim = best_seq_params[\"emb_dim\"]\n",
    "cfg.lstm_hidden = best_seq_params[\"hidden\"]\n",
    "cfg.dropout = best_seq_params[\"dropout\"]\n",
    "\n",
    "attn_model = BiLSTMAttention(vocab_size=len(stoi), emb_dim=cfg.emb_dim, hidden=cfg.lstm_hidden,\n",
    "                            layers=cfg.lstm_layers, dropout=cfg.dropout)\n",
    "attn_model = fit_seq(attn_model, seq_train_loader, seq_val_loader, epochs=cfg.epochs)\n",
    "\n",
    "yv, pv, _ = eval_seq(attn_model, seq_val_loader)\n",
    "yt, pt, _ = eval_seq(attn_model, seq_test_loader)\n",
    "\n",
    "attn_t, _, metrics_attn_test = eval_with_threshold(\n",
    "    \"BiLSTM+Attention\", yv, pv, yt, pt, show_report=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9a95a",
   "metadata": {},
   "source": [
    "## Autoencoder (MSE) on TF-IDF + classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacab13",
   "metadata": {},
   "source": [
    "### Autoencoder on TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2031e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfAutoencoder(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck=256):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, bottleneck),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(bottleneck, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, in_dim)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        xhat = self.dec(z)\n",
    "        return z, xhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a865117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the TF‑IDF feature count to keep RAM reasonable\n",
    "tfidf_ae = TfidfVectorizer(max_features=20_000, ngram_range=(1,2), min_df=2)\n",
    "Xtr_ae = tfidf_ae.fit_transform(X_train_p)\n",
    "Xva_ae = tfidf_ae.transform(X_val_p)\n",
    "Xte_ae = tfidf_ae.transform(X_test_p)\n",
    "\n",
    "ae_train = SparseTfidfDataset(Xtr_ae, y_train)\n",
    "ae_val   = SparseTfidfDataset(Xva_ae, y_val)\n",
    "ae_test  = SparseTfidfDataset(Xte_ae, y_test)\n",
    "\n",
    "ae_train_loader = DataLoader(ae_train, batch_size=256, shuffle=True)\n",
    "ae_val_loader   = DataLoader(ae_val, batch_size=256, shuffle=False)\n",
    "ae_test_loader  = DataLoader(ae_test, batch_size=256, shuffle=False)\n",
    "\n",
    "in_dim = Xtr_ae.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73aa54ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE epoch 1/3 mse=0.000050\n",
      "AE epoch 2/3 mse=0.000050\n",
      "AE epoch 3/3 mse=0.000050\n"
     ]
    }
   ],
   "source": [
    "def train_autoencoder(ae, loader, epochs=3):\n",
    "    ae = ae.to(device)\n",
    "    opt = torch.optim.Adam(ae.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        ae.train()\n",
    "        total = 0.0\n",
    "        n = 0\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            z, xhat = ae(x)\n",
    "            loss = loss_fn(xhat, x)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * x.size(0)\n",
    "            n += x.size(0)\n",
    "        print(f\"AE epoch {ep}/{epochs} mse={total/n:.6f}\")\n",
    "\n",
    "    return ae\n",
    "\n",
    "ae = TfidfAutoencoder(in_dim=in_dim, bottleneck=256)\n",
    "ae = train_autoencoder(ae, ae_train_loader, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e0715",
   "metadata": {},
   "source": [
    "### Encode TF-IDF through AE and train a classifier on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4076b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE-Embeddings + LogReg best threshold (val) = 0.05, f1=0.1495\n",
      "AE-Embeddings + LogReg (val @t=0.05): acc=0.0808 precision=0.0808 recall=1.0000 f1=0.1495\n",
      "AE-Embeddings + LogReg (test @t=0.05): acc=0.0805 precision=0.0805 recall=1.0000 f1=0.1489\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def encode_dataset(ae, loader):\n",
    "    ae.eval()\n",
    "    Z, Y = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        z, _ = ae(x)\n",
    "        Z.append(z.cpu().numpy())\n",
    "        Y.append(y.numpy())\n",
    "    return np.vstack(Z), np.concatenate(Y).astype(int)\n",
    "\n",
    "Ztr, Ytr = encode_dataset(ae, ae_train_loader)\n",
    "Zva, Yva = encode_dataset(ae, ae_val_loader)\n",
    "Zte, Yte = encode_dataset(ae, ae_test_loader)\n",
    "\n",
    "clf = LogisticRegression(max_iter=300, class_weight=\"balanced\")\n",
    "clf.fit(Ztr, Ytr)\n",
    "\n",
    "val_prob = clf.predict_proba(Zva)[:, 1]\n",
    "test_prob = clf.predict_proba(Zte)[:, 1]\n",
    "\n",
    "ae_t, _, metrics_ae_test = eval_with_threshold(\n",
    "    \"AE-Embeddings + LogReg\", Yva, val_prob, Yte, test_prob\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac0ca6",
   "metadata": {},
   "source": [
    "## Final comparison table - all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "da103500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>threshold</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.938778</td>\n",
       "      <td>0.623149</td>\n",
       "      <td>0.604833</td>\n",
       "      <td>0.613855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF+LogReg</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.940354</td>\n",
       "      <td>0.642241</td>\n",
       "      <td>0.583932</td>\n",
       "      <td>0.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BiLSTM+Attention</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.928267</td>\n",
       "      <td>0.544243</td>\n",
       "      <td>0.666884</td>\n",
       "      <td>0.599354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOW+LogReg</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.927689</td>\n",
       "      <td>0.541915</td>\n",
       "      <td>0.654474</td>\n",
       "      <td>0.592899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TFIDF+MLP</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.936781</td>\n",
       "      <td>0.622754</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>0.580398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AE+LogReg</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.080456</td>\n",
       "      <td>0.080456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.148930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  threshold       acc  precision    recall        f1\n",
       "3              LSTM       0.85  0.938778   0.623149  0.604833  0.613855\n",
       "1      TFIDF+LogReg       0.80  0.940354   0.642241  0.583932  0.611700\n",
       "4  BiLSTM+Attention       0.75  0.928267   0.544243  0.666884  0.599354\n",
       "0        BOW+LogReg       0.60  0.927689   0.541915  0.654474  0.592899\n",
       "2         TFIDF+MLP       0.65  0.936781   0.622754  0.543436  0.580398\n",
       "5         AE+LogReg       0.05  0.080456   0.080456  1.000000  0.148930"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "def add(name, metrics, threshold):\n",
    "    rows.append({\"model\": name, \"threshold\": threshold, **metrics})\n",
    "\n",
    "add(\"BOW+LogReg\", metrics_bow_test, bow_t)\n",
    "add(\"TFIDF+LogReg\", metrics_logreg_test, tfidf_t)\n",
    "add(\"TFIDF+MLP\", metrics_mlp_test, mlp_t)\n",
    "add(\"LSTM\", metrics_lstm_test, lstm_t)\n",
    "add(\"BiLSTM+Attention\", metrics_attn_test, attn_t)\n",
    "add(\"AE+LogReg\", metrics_ae_test, ae_t)\n",
    "\n",
    "pd.DataFrame(rows).sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21805c11",
   "metadata": {},
   "source": [
    "### Model Performance After some Training \"Improvements\" - I got even worse results.\n",
    "\n",
    "| Model               | Threshold | Accuracy | Precision | Recall | F1-score |\n",
    "|---------------------|-----------|----------|-----------|--------|----------|\n",
    "| **TF-IDF + LogReg** | 0.70      | **0.936833** | **0.607166** | 0.608752 | **0.607958** |\n",
    "| **BoW + LogReg**    | 0.60      | 0.927689 | 0.541915  | 0.654474 | 0.592899 |\n",
    "| **BiLSTM + Attention** | 0.95  | 0.919807 | 0.501176  | 0.695624 | 0.582604 |\n",
    "| **TF-IDF + MLP**    | 0.50      | 0.929581 | 0.564571  | 0.545395 | 0.554817 |\n",
    "| **LSTM**            | 0.95      | 0.886226 | 0.397011  | **0.798171** | 0.530267 |\n",
    "| **AE + LogReg**     | 0.05      | 0.080456 | 0.080456  | **1.000000** | 0.148930 |\n",
    "\n",
    "---\n",
    "\n",
    "### Changes Made Compared to the Original Version\n",
    "\n",
    "### Training & Optimization Improvements\n",
    "- Increased training epochs from **3 → 8** for neural models, with **early stopping** enabled.\n",
    "- Added **class-weighted loss (`pos_weight`)** to LSTM training to address class imbalance.\n",
    "- Introduced **early stopping based on validation performance** for sequence models.\n",
    "- Applied **learning rate scheduling** using *ReduceLROnPlateau* for LSTM training.\n",
    "- Enabled **gradient clipping** to stabilize LSTM optimization.\n",
    "\n",
    "### Optuna Enhancements (LSTM)\n",
    "- Improved Optuna objective from **validation loss → Average Precision (AP)**.\n",
    "- Expanded Optuna search space to include:\n",
    "  - Learning rate  \n",
    "  - Weight decay  \n",
    "  - Batch size  \n",
    "  - Use of class-weighted loss\n",
    "- Enabled **Optuna pruning** to terminate unpromising trials early.\n",
    "- Retrained the final **LSTM** model using the **best Optuna-selected hyperparameters**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

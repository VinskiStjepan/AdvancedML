{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d74a02",
   "metadata": {},
   "source": [
    "# Emotion-Aware Toxicity Detection\n",
    "\n",
    "This project looks at how to automatically spot harmful comments on social media by combining standard toxicity indicators with emotional signals. I use the Jigsaw Unintended Bias dataset and mix simple TF–IDF text features with GoEmotions embeddings, which capture feelings like anger, disgust, or contempt. The idea is to check how these emotions relate to toxic language and whether they help the model make better decisions. Besides accuracy, I also look at fairness by examining how the model behaves on comments tied to different identity groups. This shows where adding emotional information helps and where it might cause problems. In the end, the project produces a small but useful pipeline that tests whether emotional context can improve harmful-content detection while cutting down on biased false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06378ad7",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Sanity Checks\n",
    "\n",
    "I load train.csv, set up the identity columns, peek at a few rows and the shape, check the average toxicity and missing texts, add a text_len helper to see how long comments are, look at how often each identity flag shows up, and summarize the target distribution plus the share above 0.5 to see the imbalance. In the end I remove rows without comment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "760207f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   bisexual  black  buddhist  christian  female  heterosexual  hindu  \\\n",
       "0       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "1       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "2       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "3       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "4       0.0    0.0       0.0        0.0     0.0           0.0    0.0   \n",
       "\n",
       "   homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  \\\n",
       "0                        NaN                                  NaN     NaN   \n",
       "1                        NaN                                  NaN     NaN   \n",
       "2                        NaN                                  NaN     NaN   \n",
       "3                        NaN                                  NaN     NaN   \n",
       "4                        0.0                                 0.25     0.0   \n",
       "\n",
       "   latino  male  muslim  other_disability  other_gender  \\\n",
       "0     NaN   NaN     NaN               NaN           NaN   \n",
       "1     NaN   NaN     NaN               NaN           NaN   \n",
       "2     NaN   NaN     NaN               NaN           NaN   \n",
       "3     NaN   NaN     NaN               NaN           NaN   \n",
       "4     0.0   0.0     0.0               0.0           0.0   \n",
       "\n",
       "   other_race_or_ethnicity  other_religion  other_sexual_orientation  \\\n",
       "0                      NaN             NaN                       NaN   \n",
       "1                      NaN             NaN                       NaN   \n",
       "2                      NaN             NaN                       NaN   \n",
       "3                      NaN             NaN                       NaN   \n",
       "4                      0.0             0.0                       0.0   \n",
       "\n",
       "   physical_disability  psychiatric_or_mental_illness  transgender  white  \\\n",
       "0                  NaN                            NaN          NaN    NaN   \n",
       "1                  NaN                            NaN          NaN    NaN   \n",
       "2                  NaN                            NaN          NaN    NaN   \n",
       "3                  NaN                            NaN          NaN    NaN   \n",
       "4                  0.0                            0.0          0.0    0.0   \n",
       "\n",
       "                    created_date  publication_id  parent_id  article_id  \\\n",
       "0  2015-09-29 10:50:41.987077+00               2        NaN        2006   \n",
       "1  2015-09-29 10:50:42.870083+00               2        NaN        2006   \n",
       "2  2015-09-29 10:50:45.222647+00               2        NaN        2006   \n",
       "3  2015-09-29 10:50:47.601894+00               2        NaN        2006   \n",
       "4  2015-09-29 10:50:48.488476+00               2        NaN        2006   \n",
       "\n",
       "     rating  funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "0  rejected      0    0    0      0         0              0.0   \n",
       "1  rejected      0    0    0      0         0              0.0   \n",
       "2  rejected      0    0    0      0         0              0.0   \n",
       "3  rejected      0    0    0      0         0              0.0   \n",
       "4  rejected      0    0    0      1         0              0.0   \n",
       "\n",
       "   identity_annotator_count  toxicity_annotator_count  \n",
       "0                         0                         4  \n",
       "1                         0                         4  \n",
       "2                         0                         4  \n",
       "3                         0                         4  \n",
       "4                         4                        47  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1804874, 45)\n",
      "Target mean (toxicity): 0.103\n",
      "Missing comment_text: 3\n",
      "count    1804871.000\n",
      "mean         297.235\n",
      "std          269.197\n",
      "min            1.000\n",
      "25%           94.000\n",
      "50%          202.000\n",
      "75%          414.000\n",
      "max         1906.000\n",
      "Name: text_len, dtype: float64\n",
      "Top identity indicators by mean:\n",
      "female                           0.128\n",
      "male                             0.109\n",
      "christian                        0.095\n",
      "white                            0.057\n",
      "muslim                           0.049\n",
      "black                            0.034\n",
      "homosexual_gay_or_lesbian        0.026\n",
      "jewish                           0.018\n",
      "psychiatric_or_mental_illness    0.012\n",
      "asian                            0.012\n",
      "dtype: float64\n",
      "count    1804874.000\n",
      "mean           0.103\n",
      "std            0.197\n",
      "min            0.000\n",
      "25%            0.000\n",
      "50%            0.000\n",
      "75%            0.167\n",
      "max            1.000\n",
      "Name: target, dtype: float64\n",
      "Targets > 0.5: 0.059\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Used later for fairness and bias analysis. Each value is between 0 and 1, showing how strongly the comment refers to that identity\n",
    "identity_cols = [\n",
    "    'asian','atheist','bisexual','black','buddhist','christian','female','heterosexual','hindu',\n",
    "    'homosexual_gay_or_lesbian','intellectual_or_learning_disability','jewish','latino','male','muslim',\n",
    "    'other_disability','other_gender','other_race_or_ethnicity','other_religion',\n",
    "    'other_sexual_orientation','physical_disability','psychiatric_or_mental_illness','transgender','white'\n",
    "]\n",
    "\n",
    "display(train_df.head())\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "\n",
    "# average toxicity score.\n",
    "print(f\"Target mean (toxicity): {train_df['target'].mean():.3f}\")\n",
    "\n",
    "print(f\"Missing comment_text: {train_df['comment_text'].isna().sum()}\")\n",
    "\n",
    "# number of characters in each comment and statistic description\n",
    "train_df[\"text_len\"] = train_df[\"comment_text\"].str.len()\n",
    "print(train_df[\"text_len\"].describe().round(3))\n",
    "\n",
    "# statistics for identity columns\n",
    "iden_means = train_df[identity_cols].mean().sort_values(ascending=False)\n",
    "print(\"Top identity indicators by mean:\")\n",
    "print(iden_means.head(10).round(3))\n",
    "\n",
    "# if > 0.5 it is toxic\n",
    "print(train_df[\"target\"].describe().round(3))\n",
    "print(f\"Targets > 0.5: {(train_df['target'] > 0.5).mean():.3f}\")\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"comment_text\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50cc3c9",
   "metadata": {},
   "source": [
    "There are about 1.8M rows with 45 columns, only 3 comments are missing text.\n",
    "Top identity indicators: For example, female 0.128 means annotators tagged about 12.8% of comments as referring to “female.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd0b39",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Train/Validation Split\n",
    "\n",
    "Convert the continuous toxicity score (target) into a binary label: 0 for non-toxic and 1 for toxic. Split the data into training (90%) and validation (10%) sets, making sure the class balance (ratio of toxic to non-toxic comments) stays the same in both splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3df48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# results will be the same every run\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# produce the same sequence of values every time\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Same input with same weights give same output every run\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# dont use fastest (different) algorithm on every run - fixed same kerner and algorithm\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Remove NaNs in comment_text\"\n",
    "train_df = train_df.dropna(subset=[\"comment_text\"]).reset_index(drop=True)\n",
    "\n",
    "# binary classification label where 0 = not toxic, 1 = toxic\n",
    "train_df[\"label\"] = (train_df[\"target\"] > 0.5).astype(int)\n",
    "\n",
    "# Splits dataset into 90% train and 10% validation.\n",
    "train_text, val_text, y_train, y_val = train_test_split(\n",
    "    train_df[\"comment_text\"], # x (text) - all comment texts\n",
    "    train_df[\"label\"], # y - all labels (0 or 1)\n",
    "    test_size=0.1,\n",
    "    stratify=train_df[\"label\"], # keeps toxicity ratio equal in both splits\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a83ed",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression Baseline with TF–IDF Features\n",
    "\n",
    "Raw comment text is converted into numeric features using a TF–IDF vectorizer, which captures both single words and word pairs while filtering out very rare terms. These features are then used to train a Logistic Regression classifier that predicts whether a comment is toxic or not. After training, the model outputs toxicity probabilities for the validation set, which are turned into binary predictions using a 0.5 threshold. Evaluate how well the model works using three metrics: ROC-AUC (how well it ranks toxic vs non-toxic), PR-AUC (how well it detects the toxic class in an imbalanced setting), and F1-score at 0.5 (the balance between precision and recall at the chosen decision threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805e4594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC:        0.959\n",
      "PR-AUC:         0.709\n",
      "F1@0.50:        0.578\n",
      "Best F1:        0.648  (threshold=0.80)\n"
     ]
    }
   ],
   "source": [
    "# definition of TF–IDF vectorizer - converts raw text → numerical feature vectors that ML models can understand\n",
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=True, # converts all text to lowercase (dog will be same as Dog)\n",
    "    ngram_range=(1, 2), # insults can be phrases, so I use unigrams (single words) and bigrams (two-word phrases)\n",
    "    min_df=5, # ignore words that appear in fewer than 5 documents\n",
    "    max_features=200_000, # vocabulary size limit to the top 200k tokens (so we dont get over milion features)\n",
    "    strip_accents=\"unicode\", # converts accented characters\n",
    "    dtype=np.float32 # save memory\n",
    ")\n",
    "\n",
    "# learn vocabulary (train) from train_text and converts text into a sparse matrix\n",
    "X_train = tfidf.fit_transform(train_text)\n",
    "\n",
    "# only transform, we use same vocabulary from train text\n",
    "X_val   = tfidf.transform(val_text)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    solver=\"liblinear\",        # very reliable for TF–IDF where I have lots of features (200k)\n",
    "    C=1.0,                     # regularization strength (low -> strict, high -> not strict)\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# train\n",
    "clf.fit(X_train, y_train)\n",
    "val_scores = clf.predict_proba(X_val)[:, 1]\n",
    "val_preds_05 = (val_scores >= 0.5).astype(int)\n",
    "\n",
    "roc  = roc_auc_score(y_val, val_scores)\n",
    "prauc = average_precision_score(y_val, val_scores)\n",
    "f1_05 = f1_score(y_val, val_preds_05)\n",
    "\n",
    "# Find threshold that maximizes F1 on validation\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "f1s = [f1_score(y_val, (val_scores >= t).astype(int)) for t in thresholds]\n",
    "best_idx = int(np.argmax(f1s))\n",
    "best_t = float(thresholds[best_idx])\n",
    "best_f1 = float(f1s[best_idx])\n",
    "\n",
    "print(f\"ROC-AUC:        {roc:.3f}\")\n",
    "print(f\"PR-AUC:         {prauc:.3f}\")\n",
    "print(f\"F1@0.50:        {f1_05:.3f}\")\n",
    "print(f\"Best F1:        {best_f1:.3f}  (threshold={best_t:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f6a6e",
   "metadata": {},
   "source": [
    "Logistic Regression scores: \n",
    "\n",
    "ROC-AUC: 0.959\n",
    "The model separates toxic from non-toxic comments very well. It assigns higher toxicity scores to harmful comments with high consistency.\n",
    "\n",
    "PR-AUC: 0.708\n",
    "This shows strong performance on the toxic class, which is rare in the dataset. The model can detect toxic comments reasonably well without too many false positives.\n",
    "\n",
    "F1 (0.5): 0.574\n",
    "At the default 0.5 threshold, the balance between precision and recall is moderate. The score suggests that the threshold may not be ideal and could be tuned to improve performance.\n",
    "\n",
    "F1 score is best using 0.8 threshold because comments that are truly toxic often get very high probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359782ae",
   "metadata": {},
   "source": [
    "## 4. DistilBERT Toxicity Classification Baseline \n",
    "\n",
    "Unlike TF–IDF, which relies on surface-level word frequency, DistilBERT captures semantics such as intent, phrasing, and implicit meaning. Although the main baseline in this project is a TF–IDF + Logistic Regression model, DistilBERT is included as a reference baseline to show how much performance can be gained by using a modern deep language model that does not rely on handcrafted features or emotional signals (Result is after core part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0edc55a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# train_df: must have \"comment_text\" (no NaNs) and \"target\"\\ntrain_df = train_df.dropna(subset=[\"comment_text\"]).reset_index(drop=True)\\ntrain_df[\"label\"] = (train_df[\"target\"] > 0.5).astype(int)\\n\\n# If dataset is smaller than N_SUBSAMPLE, just use all\\nif len(train_df) > N_SUBSAMPLE:\\n    sample_df, _ = train_test_split(\\n        train_df,\\n        train_size=N_SUBSAMPLE,\\n        stratify=train_df[\"label\"],\\n        random_state=RANDOM_SEED\\n    )\\nelse:\\n    sample_df = train_df\\n\\ntrain_text, val_text, y_train, y_val = train_test_split(\\n    sample_df[\"comment_text\"],\\n    sample_df[\"label\"],\\n    test_size=0.1,\\n    stratify=sample_df[\"label\"],\\n    random_state=RANDOM_SEED,\\n)\\n\\n# Model and tokenizer setup\\n\\nmodel_name = \"distilbert-base-uncased\"\\nmax_length = 128\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\nclass TextDataset(Dataset):\\n    def __init__(self, texts, labels):\\n        self.texts = list(texts)\\n        self.labels = list(labels)\\n\\n    def __len__(self):\\n        return len(self.labels)\\n\\n    def __getitem__(self, idx):\\n        enc = tokenizer(\\n            self.texts[idx],\\n            padding=\"max_length\",\\n            truncation=True,\\n            max_length=max_length,\\n            return_tensors=\"pt\",\\n        )\\n        return {\\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\\n            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\\n        }\\n\\ntrain_ds = TextDataset(train_text, y_train)\\nval_ds   = TextDataset(val_text, y_val)\\n\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    model_name,\\n    num_labels=2,\\n    problem_type=\"single_label_classification\",\\n)\\n\\n# Metrics and training arguments\\n\\ndef compute_metrics(eval_pred):\\n    logits, labels = eval_pred\\n    probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\\n    preds = (probs > 0.5).astype(int)\\n    return {\\n        \"roc_auc\": roc_auc_score(labels, probs),\\n        \"pr_auc\":  average_precision_score(labels, probs),\\n        \"f1_0.5\":  f1_score(labels, preds),\\n    }\\n\\ntraining_args = TrainingArguments(\\n    output_dir=\"./bert-toxic-out\",\\n    evaluation_strategy=\"epoch\",  # in new HF versions: eval_strategy=\"epoch\"\\n    save_strategy=\"no\",\\n    per_device_train_batch_size=16,\\n    per_device_eval_batch_size=32,\\n    num_train_epochs=2,\\n    learning_rate=2e-5,\\n    weight_decay=0.01,\\n    logging_steps=200,\\n    load_best_model_at_end=False,\\n    fp16=torch.cuda.is_available(),  # automatically use GPU mixed precision if available\\n)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_ds,\\n    eval_dataset=val_ds,\\n    compute_metrics=compute_metrics,\\n)\\n\\n# Train\\n\\ntrainer.train()\\n\\n# Validation and threshold tuning\\n\\nval_logits = trainer.predict(val_ds).predictions\\nval_probs = torch.softmax(torch.tensor(val_logits), dim=1)[:, 1].numpy()\\n\\nthresholds = np.linspace(0.1, 0.9, 17)\\nf1s = [f1_score(y_val, val_probs > t) for t in thresholds]\\nbest_t = thresholds[int(np.argmax(f1s))]\\n\\nprint(f\"Best threshold: {best_t:.3f} | F1: {max(f1s):.3f}\")\\nprint(f\"ROC-AUC: {roc_auc_score(y_val, val_probs):.3f}\")\\nprint(f\"PR-AUC:  {average_precision_score(y_val, val_probs):.3f}\")\\nprint(f\"F1@0.5:  {f1_score(y_val, val_probs > 0.5):.3f}\")\\nprint(f\"F1@best: {max(f1s):.3f}\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# train_df: must have \"comment_text\" (no NaNs) and \"target\"\n",
    "train_df = train_df.dropna(subset=[\"comment_text\"]).reset_index(drop=True)\n",
    "train_df[\"label\"] = (train_df[\"target\"] > 0.5).astype(int)\n",
    "\n",
    "N_SUBSAMPLE = 200_000 \n",
    "\n",
    "# If dataset is smaller than N_SUBSAMPLE, just use all\n",
    "if len(train_df) > N_SUBSAMPLE:\n",
    "    train_df, _ = train_test_split(\n",
    "        train_df,\n",
    "        train_size=N_SUBSAMPLE,\n",
    "        stratify=train_df[\"label\"],\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "train_text, val_text, y_train, y_val = train_test_split(\n",
    "    train_df[\"comment_text\"],\n",
    "    train_df[\"label\"],\n",
    "    test_size=0.1,\n",
    "    stratify=train_df[\"label\"],\n",
    "    random_state=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# Model and tokenizer setup\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "max_length = 128\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_ds = TextDataset(train_text, y_train)\n",
    "val_ds   = TextDataset(val_text, y_val)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",\n",
    ")\n",
    "\n",
    "# Metrics and training arguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    return {\n",
    "        \"roc_auc\": roc_auc_score(labels, probs),\n",
    "        \"pr_auc\":  average_precision_score(labels, probs),\n",
    "        \"f1_0.5\":  f1_score(labels, preds),\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-toxic-out\",\n",
    "    evaluation_strategy=\"epoch\",  # in new HF versions: eval_strategy=\"epoch\"\n",
    "    save_strategy=\"no\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=torch.cuda.is_available(),  # automatically use GPU mixed precision if available\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Validation and threshold tuning\n",
    "\n",
    "val_logits = trainer.predict(val_ds).predictions\n",
    "val_probs = torch.softmax(torch.tensor(val_logits), dim=1)[:, 1].numpy()\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 17)\n",
    "f1s = [f1_score(y_val, val_probs > t) for t in thresholds]\n",
    "best_t = thresholds[int(np.argmax(f1s))]\n",
    "\n",
    "print(f\"Best threshold: {best_t:.3f} | F1: {max(f1s):.3f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, val_probs):.3f}\")\n",
    "print(f\"PR-AUC:  {average_precision_score(y_val, val_probs):.3f}\")\n",
    "print(f\"F1@0.5:  {f1_score(y_val, val_probs > 0.5):.3f}\")\n",
    "print(f\"F1@best: {max(f1s):.3f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca2b00",
   "metadata": {},
   "source": [
    " DistilBERT results interpretation:\n",
    "\n",
    "- Best threshold: 0.450 | F1: 0.663\n",
    "- ROC-AUC: 0.965\n",
    "- PR-AUC:  0.753\n",
    "- F1 (0.5):  0.661\n",
    "- F1 (best): 0.663\n",
    "\n",
    " DistilBERT achieves very strong performance (ROC-AUC = 0.965, PR-AUC = 0.753) and only by litle outperforms the TF–IDF + Logistic Regression baseline in terms of F1-score (using 0.8 threshold). However, the improvement over Logistic Regression is relatively small when compared to the substantially higher computational cost and training time. This highlights an important trade-off: while transformer-based models provide better semantic understanding, simpler linear models remain competitive and more efficient, motivating the exploration of emotion-aware features as a lightweight alternative to close the performance gap without the overhead of full transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ae2a1",
   "metadata": {},
   "source": [
    "## 5. Emotion Feature Extraction with GoEmotions (Affective Signals)\n",
    "\n",
    "This section extracts explicit emotion signals from each comment using a pretrained GoEmotions classifier. For every text, the model outputs a 28-dimensional probability vector (e.g., anger, disgust, contempt, neutral). These emotion probabilities are cached and later concatenated with TF–IDF features to build the emotion-aware toxicity model and to analyze fairness effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ac7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed and cached GoEmotions probabilities.\n",
      "Emotion labels (28): ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "E_train shape: (1624383, 28)\n",
      "E_val shape:   (180488, 28)\n",
      "Example (first row, top-5 emotions):\n",
      "  surprise: 0.533\n",
      "  curiosity: 0.334\n",
      "  neutral: 0.214\n",
      "  confusion: 0.030\n",
      "  excitement: 0.015\n"
     ]
    }
   ],
   "source": [
    "GOEMOTIONS_MODEL = \"SamLowe/roberta-base-go_emotions\" # RoBERTa model was trained to detect emotions like anger, disgust, fear, joy, neutral, etc.\n",
    "\n",
    "# create folder for cache\n",
    "CACHE_DIR = \"./cache_goemotions\" \n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# caching extracted emotion futures\n",
    "TRAIN_CACHE = os.path.join(CACHE_DIR, \"goemotions_probs_train.npy\")\n",
    "VAL_CACHE   = os.path.join(CACHE_DIR, \"goemotions_probs_val.npy\")\n",
    "LABELS_CACHE = os.path.join(CACHE_DIR, \"goemotions_labels.npy\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load tokenizer from pretrained model, loads neural network and move it to CPU/GPU and puts model to evaluation mode\n",
    "tokenizer_em = AutoTokenizer.from_pretrained(GOEMOTIONS_MODEL)\n",
    "model_em = AutoModelForSequenceClassification.from_pretrained(GOEMOTIONS_MODEL).to(device)\n",
    "model_em.eval()\n",
    "\n",
    "# Label names (length should be 28) - (e.g., anger, disgust, neutral)\n",
    "emotion_labels = [model_em.config.id2label[i] for i in range(model_em.config.num_labels)]\n",
    "np.save(LABELS_CACHE, np.array(emotion_labels, dtype=object))\n",
    "\n",
    "def extract_goemotions_probs(texts, batch_size=64, max_length=256):\n",
    "    \"\"\"\n",
    "    Returns an (n_samples x n_emotions) numpy array of emotion probabilities.\n",
    "    \"\"\"\n",
    "    probs_all = []\n",
    "    with torch.no_grad(): # tells PyTorch this is not training, don’t track gradients.\n",
    "        texts = list(texts)\n",
    "        for i in range(0, len(texts), batch_size): # loop over texts in batches\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer_em( \n",
    "                batch,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\", # return PyTorch tensors - i am using PyTorch\n",
    "            ).to(device) # tokenizes text batch and prepares it for the model\n",
    "\n",
    "            logits = model_em(**enc).logits # runs the model and gets raw emotion scores\n",
    "            probs = torch.sigmoid(logits).cpu().numpy() # converts logits to probabilities\n",
    "            probs_all.append(probs)\n",
    "\n",
    "    return np.vstack(probs_all) # return on big matrix of batches results\n",
    "\n",
    "# run with caching if exist\n",
    "if os.path.exists(TRAIN_CACHE) and os.path.exists(VAL_CACHE):\n",
    "    E_train = np.load(TRAIN_CACHE)\n",
    "    E_val   = np.load(VAL_CACHE)\n",
    "    print(\"Loaded cached GoEmotions probabilities.\")\n",
    "else:\n",
    "    E_train = extract_goemotions_probs(train_text, batch_size=64, max_length=256)\n",
    "    E_val   = extract_goemotions_probs(val_text, batch_size=64, max_length=256)\n",
    "\n",
    "    np.save(TRAIN_CACHE, E_train)\n",
    "    np.save(VAL_CACHE, E_val)\n",
    "    print(\"Computed and cached GoEmotions probabilities.\")\n",
    "\n",
    "print(\"Emotion labels (28):\", emotion_labels)\n",
    "print(\"E_train shape:\", E_train.shape)\n",
    "print(\"E_val shape:  \", E_val.shape)\n",
    "print(\"Example (first row, top-5 emotions):\")\n",
    "top5 = np.argsort(E_train[0])[::-1][:5]\n",
    "for idx in top5:\n",
    "    print(f\"  {emotion_labels[idx]}: {E_train[0, idx]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca9268",
   "metadata": {},
   "source": [
    "Computed and cached GoEmotions probabilities.\n",
    "Emotion labels (28): ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "E_train shape: (1624383, 28)\n",
    "E_val shape:   (180488, 28)\n",
    "Example (first row, top-5 emotions):\n",
    "  surprise: 0.533\n",
    "  curiosity: 0.334\n",
    "  neutral: 0.214\n",
    "  confusion: 0.030\n",
    "  excitement: 0.015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544a8b5",
   "metadata": {},
   "source": [
    "## 6. Emotion-Only Toxicity Classification (GoEmotions + Logistic Regression)\n",
    "\n",
    "This section trains a toxicity classifier using only emotion probabilities extracted with the GoEmotions model. Each comment is represented as a 28-dimensional emotional feature vector, and a Logistic Regression model is used to predict toxicity based on these signals alone. The goal is to evaluate how much emotional information contributes to toxicity detection without relying on textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db9a032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion-only (GoEmotions) -> LogisticRegression\n",
      "ROC-AUC: 0.8172\n",
      "PR-AUC : 0.3424\n",
      "F1@0.5 : 0.3214\n",
      "\n",
      "Confusion matrix @0.5 [ [TN FP], [FN TP] ]:\n",
      "[[146090  23754]\n",
      " [  4057   6587]]\n",
      "\n",
      "Best threshold by F1: 0.7773\n",
      "F1@best_thr        : 0.3860\n",
      "\n",
      "Confusion matrix with best thr [ [TN FP], [FN TP] ]:\n",
      "[[162480   7364]\n",
      " [  6337   4307]]\n",
      "\n",
      "Classification report @best_thr:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9625    0.9566    0.9595    169844\n",
      "           1     0.3690    0.4046    0.3860     10644\n",
      "\n",
      "    accuracy                         0.9241    180488\n",
      "   macro avg     0.6657    0.6806    0.6728    180488\n",
      "weighted avg     0.9275    0.9241    0.9257    180488\n",
      "\n",
      "\n",
      "Top emotions decreasing toxicity (most negative coefficients):\n",
      "         neutral: -0.4290\n",
      "     disapproval: -0.4118\n",
      "        approval: -0.3040\n",
      "       confusion: -0.2333\n",
      "  disappointment: -0.2329\n",
      "      admiration: -0.1905\n",
      "       gratitude: -0.1854\n",
      "       curiosity: -0.1802\n",
      "\n",
      "Top emotions increasing toxicity (most positive coefficients):\n",
      "       annoyance:  0.7092\n",
      "         sadness:  0.1204\n",
      "           anger:  0.1146\n",
      "         disgust:  0.0737\n",
      "   embarrassment:  0.0239\n",
      "            fear:  0.0131\n",
      "           pride:  0.0123\n",
      "       amusement:  0.0043\n"
     ]
    }
   ],
   "source": [
    "# I use scaler to assign fair weights to all emotions\n",
    "emo_clf = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"lr\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        solver=\"lbfgs\",\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1 if \"n_jobs\" in LogisticRegression().get_params() else None\n",
    "    ))\n",
    "])\n",
    "\n",
    "emo_clf.fit(E_train, y_train)\n",
    "\n",
    "# Predict probabilities on validation\n",
    "val_proba = emo_clf.predict_proba(E_val)[:, 1]\n",
    "\n",
    "# Basic metrics (threshold 0.5) \n",
    "val_pred_05 = (val_proba >= 0.5).astype(int)\n",
    "\n",
    "roc = roc_auc_score(y_val, val_proba)\n",
    "pr  = average_precision_score(y_val, val_proba)\n",
    "f1  = f1_score(y_val, val_pred_05)\n",
    "\n",
    "print(\"Emotion-only LogisticRegression\")\n",
    "print(f\"ROC-AUC: {roc:.4f}\")\n",
    "print(f\"PR-AUC : {pr:.4f}\")\n",
    "print(f\"F1@0.5 : {f1:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Confusion matrix @0.5 [ [TN FP], [FN TP] ]:\")\n",
    "print(confusion_matrix(y_val, val_pred_05))\n",
    "print()\n",
    "\n",
    "# choose a better threshold on validation by maximizing F1\n",
    "prec, rec, thr = precision_recall_curve(y_val, val_proba)\n",
    "# precision_recall_curve returns thr of length (len(prec)-1)\n",
    "f1s = (2 * prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "best_idx = int(np.argmax(f1s))\n",
    "best_thr = float(thr[best_idx])\n",
    "best_f1 = float(f1s[best_idx])\n",
    "\n",
    "val_pred_best = (val_proba >= best_thr).astype(int)\n",
    "\n",
    "print(f\"Best threshold by F1: {best_thr:.4f}\")\n",
    "print(f\"F1@best_thr        : {best_f1:.4f}\")\n",
    "print()\n",
    "print(\"Confusion matrix with best thr [ [TN FP], [FN TP] ]:\")\n",
    "print(confusion_matrix(y_val, val_pred_best))\n",
    "print()\n",
    "print(\"Classification report @best_thr:\")\n",
    "print(classification_report(y_val, val_pred_best, digits=4))\n",
    "\n",
    "# show which emotions the model relies on most\n",
    "# Positive coef => increases toxicity probability; negative => decreases it.\n",
    "lr = emo_clf.named_steps[\"lr\"]\n",
    "coefs = lr.coef_.ravel()\n",
    "\n",
    "if \"emotion_labels\" in globals() and len(emotion_labels) == len(coefs):\n",
    "    idx_sorted = np.argsort(coefs)\n",
    "    print(\"\\nTop emotions decreasing toxicity (most negative coefficients):\")\n",
    "    for i in idx_sorted[:8]:\n",
    "        print(f\"  {emotion_labels[i]:>14s}: {coefs[i]: .4f}\")\n",
    "\n",
    "    print(\"\\nTop emotions increasing toxicity (most positive coefficients):\")\n",
    "    for i in idx_sorted[-8:][::-1]:\n",
    "        print(f\"  {emotion_labels[i]:>14s}: {coefs[i]: .4f}\")\n",
    "else:\n",
    "    # Fallback if emotion_labels not defined\n",
    "    idx_sorted = np.argsort(coefs)\n",
    "    print(\"\\nTop coefficients (negative -> positive):\")\n",
    "    print(coefs[idx_sorted[:8]])\n",
    "    print(coefs[idx_sorted[-8:][::-1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88283c9a",
   "metadata": {},
   "source": [
    "Emotion-Only Toxicity Classification result:\n",
    "\n",
    "- ROC-AUC (0.817)  -> Higher means toxic comments generally “feel more emotional” than non-toxic ones.\n",
    "- PR-AUC (0.342)   -> Low value means many emotional comments are falsely flagged as toxic.\n",
    "- F1 @ 0.5 (0.321) -> Low score shows that emotions alone trigger too many false positives.\n",
    "- Best F1 (0.386)  -> Even then, performance remains limited.\n",
    "- Confusion matrix -> FP Emotional but harmless comments wrongly labeled as toxic, FN Toxic comments that are calm or indirect and lack strong emotion\n",
    "- Accuracy         -> is misleading because most comments are non-toxic, always predicting “non-toxic” would already give high accuracy.\n",
    "- Coefficients     -> Emotions that reduce toxicity prediction (Neutral, approval, gratitude, curiosity), are signals of normal or positive communication\n",
    "                 -> Emotions that increase toxicity prediction (Annoyance, anger, disgust), often present in toxic language, but also in non-toxic complaints\n",
    "\n",
    "The emotion-only model is sensitive to emotional intensity but lacks the semantic understanding required to distinguish harmful attacks from benign emotional expression. As a result, it produces many false positives and achieves limited F1 performance, confirming that emotional cues alone are insufficient for reliable toxicity detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5fdb5",
   "metadata": {},
   "source": [
    "## 7. TF–IDF + Emotion feature fusion classifier\n",
    "\n",
    "This section builds a fusion toxicity model by combining two types of information for each comment:\n",
    "- What is said (TF–IDF word/bigram features), and\n",
    "- How it feels (28 GoEmotions probabilities).\n",
    "\n",
    "The goal is to test whether adding emotional signals improves toxicity prediction (and later fairness), compared to using TF–IDF or emotions alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d878075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "  X_train: (1624383, 200000) E_train: (1624383, 28) => fused: (1624383, 200028)\n",
      "  X_val  : (180488, 200000) E_val  : (180488, 28) => fused: (180488, 200028)\n",
      "\n",
      "TF–IDF + Emotions (FUSED) metrics:\n",
      "ROC-AUC: 0.959\n",
      "PR-AUC:  0.717\n",
      "F1@0.5:  0.579\n",
      "Confusion @0.5:\n",
      " [[158681  11163]\n",
      " [  1765   8879]]\n",
      "\n",
      "Best threshold by F1 (fused): 0.8179\n",
      "Best F1 (fused):             0.6537\n",
      "Confusion @best_thr:\n",
      " [[165860   3984]\n",
      " [  3541   7103]]\n",
      "\n",
      "FUSED model: emotions decreasing toxicity (most negative):\n",
      "           grief: -2.0799\n",
      "     realization: -1.6744\n",
      "      excitement: -1.3661\n",
      "          relief: -1.3437\n",
      "     disapproval: -1.2050\n",
      "     nervousness: -1.0860\n",
      "       confusion: -0.9698\n",
      "        approval: -0.8710\n",
      "\n",
      "FUSED model: emotions increasing toxicity (most positive):\n",
      "       annoyance:  3.0564\n",
      "           anger:  1.6882\n",
      "         disgust:  1.3618\n",
      "   embarrassment:  0.5998\n",
      "       amusement:  0.3033\n",
      "            fear:  0.0519\n",
      "         sadness: -0.2536\n",
      "         remorse: -0.2564\n"
     ]
    }
   ],
   "source": [
    "# safety checks - same number of comments in TF–IDF and emotions\n",
    "assert X_train.shape[0] == E_train.shape[0], f\"Mismatch: X_train {X_train.shape[0]} vs E_train {E_train.shape[0]}\"\n",
    "assert X_val.shape[0]   == E_val.shape[0],   f\"Mismatch: X_val {X_val.shape[0]} vs E_val {E_val.shape[0]}\"\n",
    "assert len(y_train) == E_train.shape[0]\n",
    "assert len(y_val)   == E_val.shape[0]\n",
    "\n",
    "# convert emotions to sparse like TF–IDF and fuse with TF–IDF\n",
    "Etr_sp = sp.csr_matrix(E_train.astype(np.float32))\n",
    "Eva_sp = sp.csr_matrix(E_val.astype(np.float32))\n",
    "\n",
    "# hstack - put features side-by-side -> words + emotions together\n",
    "X_train_fused = sp.hstack([X_train, Etr_sp], format=\"csr\")\n",
    "X_val_fused   = sp.hstack([X_val,   Eva_sp], format=\"csr\")\n",
    "\n",
    "# verify dimensions, must be 200k + 28\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"E_train:\", E_train.shape, \"=> fused:\", X_train_fused.shape)\n",
    "print(\"  X_val  :\", X_val.shape,   \"E_val  :\", E_val.shape,   \"=> fused:\", X_val_fused.shape)\n",
    "\n",
    "# train fused classifier\n",
    "clf_fused = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "clf_fused.fit(X_train_fused, y_train)\n",
    "\n",
    "# evaluate fused model\n",
    "val_scores_fused = clf_fused.predict_proba(X_val_fused)[:, 1]\n",
    "val_preds_fused_05 = (val_scores_fused > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nTF–IDF + Emotions (FUSED) metrics:\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_val, val_scores_fused):.3f}\")\n",
    "print(f\"PR-AUC:  {average_precision_score(y_val, val_scores_fused):.3f}\")\n",
    "print(f\"F1@0.5:  {f1_score(y_val, val_preds_fused_05):.3f}\")\n",
    "print(\"Confusion @0.5:\\n\", confusion_matrix(y_val, val_preds_fused_05))\n",
    "\n",
    "# best threshold by F1 on validation\n",
    "prec, rec, thr = precision_recall_curve(y_val, val_scores_fused)\n",
    "f1s = (2 * prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "best_idx = int(np.argmax(f1s))\n",
    "best_thr = float(thr[best_idx])\n",
    "best_f1  = float(f1s[best_idx])\n",
    "\n",
    "val_preds_fused_best = (val_scores_fused >= best_thr).astype(int)\n",
    "\n",
    "print(f\"\\nBest threshold by F1 (fused): {best_thr:.4f}\")\n",
    "print(f\"Best F1 (fused):             {best_f1:.4f}\")\n",
    "print(\"Confusion @best_thr:\\n\", confusion_matrix(y_val, val_preds_fused_best))\n",
    "\n",
    "# inspect emotion coefficients inside the fused model\n",
    "# last 28 coefficients correspond to emotions (because we appended E_* at the end).\n",
    "if \"emotion_labels\" not in globals():\n",
    "    emotion_labels = [f\"emo_{i}\" for i in range(E_train.shape[1])]\n",
    "\n",
    "emo_coef = clf_fused.coef_.ravel()[-len(emotion_labels):]\n",
    "idx_sorted = np.argsort(emo_coef)\n",
    "\n",
    "print(\"\\nFUSED model: emotions decreasing toxicity (most negative):\")\n",
    "for i in idx_sorted[:8]:\n",
    "    print(f\"  {emotion_labels[i]:>14s}: {emo_coef[i]: .4f}\")\n",
    "\n",
    "print(\"\\nFUSED model: emotions increasing toxicity (most positive):\")\n",
    "for i in idx_sorted[-8:][::-1]:\n",
    "    print(f\"  {emotion_labels[i]:>14s}: {emo_coef[i]: .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f7a3f",
   "metadata": {},
   "source": [
    "TF–IDF + Emotion feature fusion result:\n",
    "\n",
    "Compared to the TF–IDF-only model, the fusion model achieves very similar ranking performance (ROC-AUC = 0.959 vs. 0.959), indicating that lexical features remain the dominant source of information for toxicity detection. However, the fusion model slightly improves PR-AUC (0.717 vs. ~0.709), suggesting better handling of the minority toxic class when precision and recall are jointly considered. \n",
    "The best F1 score of the fusion model (0.654) is marginally higher than the TF–IDF-only model (≈0.648), showing a small but consistent improvement when emotional features are added.\n",
    "\n",
    "Most improvements we see in Coefficients. These emotional effects are much stronger and more meaningful in the fused model than in the emotion-only model, where emotions often caused false alarms. This indicates that emotions are most useful in context, not in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99b7ac",
   "metadata": {},
   "source": [
    "## 8. Fairness by Identity Group (Baseline TF–IDF vs Fused TF–IDF+Emotions)\n",
    "\n",
    "This code compares false positives and false negatives across identity groups (e.g., male, female, black, muslim) for two models: a baseline TF–IDF model and a fused TF–IDF + emotions model. The goal is to see whether adding emotions changes bias-related errors, especially false positive rate (FPR) on identity-linked comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2ad78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness comparison (negative ΔFPR means fused reduces false positives):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>base_n</th>\n",
       "      <th>base_tox_rate</th>\n",
       "      <th>base_FPR</th>\n",
       "      <th>base_FNR</th>\n",
       "      <th>base_PPV</th>\n",
       "      <th>base_TPR</th>\n",
       "      <th>fused_n</th>\n",
       "      <th>fused_tox_rate</th>\n",
       "      <th>fused_FPR</th>\n",
       "      <th>fused_FNR</th>\n",
       "      <th>fused_PPV</th>\n",
       "      <th>fused_TPR</th>\n",
       "      <th>ΔFPR</th>\n",
       "      <th>ΔFNR</th>\n",
       "      <th>ΔPPV</th>\n",
       "      <th>ΔTPR</th>\n",
       "      <th>base_FP</th>\n",
       "      <th>fused_FP</th>\n",
       "      <th>base_FN</th>\n",
       "      <th>fused_FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>physical_disability</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black</td>\n",
       "      <td>1486</td>\n",
       "      <td>0.218708</td>\n",
       "      <td>0.409130</td>\n",
       "      <td>0.169231</td>\n",
       "      <td>0.362416</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>1486</td>\n",
       "      <td>0.218708</td>\n",
       "      <td>0.184324</td>\n",
       "      <td>0.350769</td>\n",
       "      <td>0.496471</td>\n",
       "      <td>0.649231</td>\n",
       "      <td>-0.224806</td>\n",
       "      <td>0.181538</td>\n",
       "      <td>0.134054</td>\n",
       "      <td>-0.181538</td>\n",
       "      <td>475</td>\n",
       "      <td>214</td>\n",
       "      <td>55</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>white</td>\n",
       "      <td>2417</td>\n",
       "      <td>0.194870</td>\n",
       "      <td>0.374615</td>\n",
       "      <td>0.142251</td>\n",
       "      <td>0.356575</td>\n",
       "      <td>0.857749</td>\n",
       "      <td>2417</td>\n",
       "      <td>0.194870</td>\n",
       "      <td>0.156732</td>\n",
       "      <td>0.352442</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.647558</td>\n",
       "      <td>-0.217883</td>\n",
       "      <td>0.210191</td>\n",
       "      <td>0.143425</td>\n",
       "      <td>-0.210191</td>\n",
       "      <td>729</td>\n",
       "      <td>305</td>\n",
       "      <td>67</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>heterosexual</td>\n",
       "      <td>124</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>124</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>-0.205882</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.167059</td>\n",
       "      <td>-0.227273</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.195192</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.152709</td>\n",
       "      <td>0.359833</td>\n",
       "      <td>0.847291</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.195192</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.438424</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.561576</td>\n",
       "      <td>-0.204301</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.097999</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>306</td>\n",
       "      <td>135</td>\n",
       "      <td>31</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>other_race_or_ethnicity</td>\n",
       "      <td>56</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.196078</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>muslim</td>\n",
       "      <td>2170</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.288793</td>\n",
       "      <td>0.200637</td>\n",
       "      <td>0.318933</td>\n",
       "      <td>0.799363</td>\n",
       "      <td>2170</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.100754</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.512739</td>\n",
       "      <td>-0.188039</td>\n",
       "      <td>0.286624</td>\n",
       "      <td>0.143711</td>\n",
       "      <td>-0.286624</td>\n",
       "      <td>536</td>\n",
       "      <td>187</td>\n",
       "      <td>63</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buddhist</td>\n",
       "      <td>45</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>transgender</td>\n",
       "      <td>236</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.267606</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>236</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.093897</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>-0.173709</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.085608</td>\n",
       "      <td>-0.434783</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bisexual</td>\n",
       "      <td>32</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.172414</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hindu</td>\n",
       "      <td>49</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>49</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.159091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>psychiatric_or_mental_illness</td>\n",
       "      <td>510</td>\n",
       "      <td>0.162745</td>\n",
       "      <td>0.252927</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>510</td>\n",
       "      <td>0.162745</td>\n",
       "      <td>0.096019</td>\n",
       "      <td>0.240964</td>\n",
       "      <td>0.605769</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>-0.156909</td>\n",
       "      <td>0.144578</td>\n",
       "      <td>0.195933</td>\n",
       "      <td>-0.144578</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jewish</td>\n",
       "      <td>788</td>\n",
       "      <td>0.104061</td>\n",
       "      <td>0.209632</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.308411</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>788</td>\n",
       "      <td>0.104061</td>\n",
       "      <td>0.056657</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>-0.152975</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.215398</td>\n",
       "      <td>-0.268293</td>\n",
       "      <td>148</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>latino</td>\n",
       "      <td>217</td>\n",
       "      <td>0.124424</td>\n",
       "      <td>0.257895</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>217</td>\n",
       "      <td>0.124424</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>-0.152632</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.097263</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atheist</td>\n",
       "      <td>133</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.182540</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>133</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>-0.134921</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>male</td>\n",
       "      <td>4476</td>\n",
       "      <td>0.108579</td>\n",
       "      <td>0.176441</td>\n",
       "      <td>0.137860</td>\n",
       "      <td>0.373108</td>\n",
       "      <td>0.862140</td>\n",
       "      <td>4476</td>\n",
       "      <td>0.108579</td>\n",
       "      <td>0.072431</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.528548</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.104010</td>\n",
       "      <td>0.195473</td>\n",
       "      <td>0.155440</td>\n",
       "      <td>-0.195473</td>\n",
       "      <td>704</td>\n",
       "      <td>289</td>\n",
       "      <td>67</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>5380</td>\n",
       "      <td>0.090149</td>\n",
       "      <td>0.151175</td>\n",
       "      <td>0.181443</td>\n",
       "      <td>0.349164</td>\n",
       "      <td>0.818557</td>\n",
       "      <td>5380</td>\n",
       "      <td>0.090149</td>\n",
       "      <td>0.056384</td>\n",
       "      <td>0.377320</td>\n",
       "      <td>0.522491</td>\n",
       "      <td>0.622680</td>\n",
       "      <td>-0.094791</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.173327</td>\n",
       "      <td>-0.195876</td>\n",
       "      <td>740</td>\n",
       "      <td>276</td>\n",
       "      <td>88</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asian</td>\n",
       "      <td>450</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.153659</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.350515</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>450</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.090244</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.168003</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>other_religion</td>\n",
       "      <td>40</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>40</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.081081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>christian</td>\n",
       "      <td>4057</td>\n",
       "      <td>0.055213</td>\n",
       "      <td>0.100444</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>0.306306</td>\n",
       "      <td>0.758929</td>\n",
       "      <td>4057</td>\n",
       "      <td>0.055213</td>\n",
       "      <td>0.033655</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.431718</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>-0.066788</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.125412</td>\n",
       "      <td>-0.321429</td>\n",
       "      <td>385</td>\n",
       "      <td>129</td>\n",
       "      <td>54</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL</td>\n",
       "      <td>180488</td>\n",
       "      <td>0.058973</td>\n",
       "      <td>0.065225</td>\n",
       "      <td>0.169861</td>\n",
       "      <td>0.443708</td>\n",
       "      <td>0.830139</td>\n",
       "      <td>180488</td>\n",
       "      <td>0.058973</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.332770</td>\n",
       "      <td>0.640628</td>\n",
       "      <td>0.667230</td>\n",
       "      <td>-0.041768</td>\n",
       "      <td>0.162909</td>\n",
       "      <td>0.196920</td>\n",
       "      <td>-0.162909</td>\n",
       "      <td>11078</td>\n",
       "      <td>3984</td>\n",
       "      <td>1808</td>\n",
       "      <td>3542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>other_gender</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_sexual_orientation</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>intellectual_or_learning_disability</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  group  base_n  base_tox_rate  base_FPR  \\\n",
       "20                  physical_disability       7       0.000000  0.285714   \n",
       "4                                 black    1486       0.218708  0.409130   \n",
       "23                                white    2417       0.194870  0.374615   \n",
       "8                          heterosexual     124       0.177419  0.323529   \n",
       "10            homosexual_gay_or_lesbian    1040       0.195192  0.365591   \n",
       "17              other_race_or_ethnicity      56       0.089286  0.274510   \n",
       "15                               muslim    2170       0.144700  0.288793   \n",
       "5                              buddhist      45       0.022222  0.272727   \n",
       "22                          transgender     236       0.097458  0.267606   \n",
       "3                              bisexual      32       0.093750  0.379310   \n",
       "9                                 hindu      49       0.102041  0.204545   \n",
       "21        psychiatric_or_mental_illness     510       0.162745  0.252927   \n",
       "12                               jewish     788       0.104061  0.209632   \n",
       "13                               latino     217       0.124424  0.257895   \n",
       "2                               atheist     133       0.052632  0.182540   \n",
       "14                                 male    4476       0.108579  0.176441   \n",
       "7                                female    5380       0.090149  0.151175   \n",
       "1                                 asian     450       0.088889  0.153659   \n",
       "18                       other_religion      40       0.075000  0.162162   \n",
       "6                             christian    4057       0.055213  0.100444   \n",
       "0                                   ALL  180488       0.058973  0.065225   \n",
       "16                         other_gender       2       0.000000  0.000000   \n",
       "19             other_sexual_orientation       3       0.666667  0.000000   \n",
       "11  intellectual_or_learning_disability      13       0.000000  0.000000   \n",
       "\n",
       "    base_FNR  base_PPV  base_TPR  fused_n  fused_tox_rate  fused_FPR  \\\n",
       "20       NaN  0.000000       NaN        7        0.000000   0.000000   \n",
       "4   0.169231  0.362416  0.830769     1486        0.218708   0.184324   \n",
       "23  0.142251  0.356575  0.857749     2417        0.194870   0.156732   \n",
       "8   0.181818  0.352941  0.818182      124        0.177419   0.117647   \n",
       "10  0.152709  0.359833  0.847291     1040        0.195192   0.161290   \n",
       "17  0.000000  0.263158  1.000000       56        0.089286   0.078431   \n",
       "15  0.200637  0.318933  0.799363     2170        0.144700   0.100754   \n",
       "5   1.000000  0.000000  0.000000       45        0.022222   0.090909   \n",
       "22  0.086957  0.269231  0.913043      236        0.097458   0.093897   \n",
       "3   0.000000  0.214286  1.000000       32        0.093750   0.206897   \n",
       "9   0.600000  0.181818  0.400000       49        0.102041   0.045455   \n",
       "21  0.096386  0.409836  0.903614      510        0.162745   0.096019   \n",
       "12  0.195122  0.308411  0.804878      788        0.104061   0.056657   \n",
       "13  0.370370  0.257576  0.629630      217        0.124424   0.105263   \n",
       "2   0.285714  0.178571  0.714286      133        0.052632   0.047619   \n",
       "14  0.137860  0.373108  0.862140     4476        0.108579   0.072431   \n",
       "7   0.181443  0.349164  0.818557     5380        0.090149   0.056384   \n",
       "1   0.150000  0.350515  0.850000      450        0.088889   0.063415   \n",
       "18  0.333333  0.250000  0.666667       40        0.075000   0.081081   \n",
       "6   0.241071  0.306306  0.758929     4057        0.055213   0.033655   \n",
       "0   0.169861  0.443708  0.830139   180488        0.058973   0.023457   \n",
       "16       NaN       NaN       NaN        2        0.000000   0.000000   \n",
       "19  0.500000  1.000000  0.500000        3        0.666667   0.000000   \n",
       "11       NaN       NaN       NaN       13        0.000000   0.000000   \n",
       "\n",
       "    fused_FNR  fused_PPV  fused_TPR      ΔFPR      ΔFNR      ΔPPV      ΔTPR  \\\n",
       "20        NaN        NaN        NaN -0.285714       NaN       NaN       NaN   \n",
       "4    0.350769   0.496471   0.649231 -0.224806  0.181538  0.134054 -0.181538   \n",
       "23   0.352442   0.500000   0.647558 -0.217883  0.210191  0.143425 -0.210191   \n",
       "8    0.409091   0.520000   0.590909 -0.205882  0.227273  0.167059 -0.227273   \n",
       "10   0.438424   0.457831   0.561576 -0.204301  0.285714  0.097999 -0.285714   \n",
       "17   0.200000   0.500000   0.800000 -0.196078  0.200000  0.236842 -0.200000   \n",
       "15   0.487261   0.462644   0.512739 -0.188039  0.286624  0.143711 -0.286624   \n",
       "5    1.000000   0.000000   0.000000 -0.181818  0.000000  0.000000  0.000000   \n",
       "22   0.521739   0.354839   0.478261 -0.173709  0.434783  0.085608 -0.434783   \n",
       "3    0.333333   0.250000   0.666667 -0.172414  0.333333  0.035714 -0.333333   \n",
       "9    0.600000   0.500000   0.400000 -0.159091  0.000000  0.318182  0.000000   \n",
       "21   0.240964   0.605769   0.759036 -0.156909  0.144578  0.195933 -0.144578   \n",
       "12   0.463415   0.523810   0.536585 -0.152975  0.268293  0.215398 -0.268293   \n",
       "13   0.592593   0.354839   0.407407 -0.152632  0.222222  0.097263 -0.222222   \n",
       "2    0.571429   0.333333   0.428571 -0.134921  0.285714  0.154762 -0.285714   \n",
       "14   0.333333   0.528548   0.666667 -0.104010  0.195473  0.155440 -0.195473   \n",
       "7    0.377320   0.522491   0.622680 -0.094791  0.195876  0.173327 -0.195876   \n",
       "1    0.300000   0.518519   0.700000 -0.090244  0.150000  0.168003 -0.150000   \n",
       "18   0.333333   0.400000   0.666667 -0.081081  0.000000  0.150000  0.000000   \n",
       "6    0.562500   0.431718   0.437500 -0.066788  0.321429  0.125412 -0.321429   \n",
       "0    0.332770   0.640628   0.667230 -0.041768  0.162909  0.196920 -0.162909   \n",
       "16        NaN        NaN        NaN  0.000000       NaN       NaN       NaN   \n",
       "19   1.000000        NaN   0.000000  0.000000  0.500000       NaN -0.500000   \n",
       "11        NaN        NaN        NaN  0.000000       NaN       NaN       NaN   \n",
       "\n",
       "    base_FP  fused_FP  base_FN  fused_FN  \n",
       "20        2         0        0         0  \n",
       "4       475       214       55       114  \n",
       "23      729       305       67       166  \n",
       "8        33        12        4         9  \n",
       "10      306       135       31        89  \n",
       "17       14         4        0         1  \n",
       "15      536       187       63       153  \n",
       "5        12         4        1         1  \n",
       "22       57        20        2        12  \n",
       "3        11         6        0         1  \n",
       "9         9         2        3         3  \n",
       "21      108        41        8        20  \n",
       "12      148        40       16        38  \n",
       "13       49        20       10        16  \n",
       "2        23         6        2         4  \n",
       "14      704       289       67       162  \n",
       "7       740       276       88       183  \n",
       "1        63        26        6        12  \n",
       "18        6         3        1         1  \n",
       "6       385       129       54       126  \n",
       "0     11078      3984     1808      3542  \n",
       "16        0         0        0         0  \n",
       "19        0         0        1         2  \n",
       "11        0         0        0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thresholds used:\n",
      "  BASE_THR : 0.5\n",
      "  FUSED_THR: 0.8179\n",
      "\n",
      "Top 10 groups with biggest FPR reduction (fused - base):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>ΔFPR</th>\n",
       "      <th>base_FPR</th>\n",
       "      <th>fused_FPR</th>\n",
       "      <th>base_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>physical_disability</td>\n",
       "      <td>-0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>black</td>\n",
       "      <td>-0.224806</td>\n",
       "      <td>0.409130</td>\n",
       "      <td>0.184324</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>white</td>\n",
       "      <td>-0.217883</td>\n",
       "      <td>0.374615</td>\n",
       "      <td>0.156732</td>\n",
       "      <td>2417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>heterosexual</td>\n",
       "      <td>-0.205882</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>homosexual_gay_or_lesbian</td>\n",
       "      <td>-0.204301</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>other_race_or_ethnicity</td>\n",
       "      <td>-0.196078</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>muslim</td>\n",
       "      <td>-0.188039</td>\n",
       "      <td>0.288793</td>\n",
       "      <td>0.100754</td>\n",
       "      <td>2170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buddhist</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>transgender</td>\n",
       "      <td>-0.173709</td>\n",
       "      <td>0.267606</td>\n",
       "      <td>0.093897</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bisexual</td>\n",
       "      <td>-0.172414</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        group      ΔFPR  base_FPR  fused_FPR  base_n\n",
       "20        physical_disability -0.285714  0.285714   0.000000       7\n",
       "4                       black -0.224806  0.409130   0.184324    1486\n",
       "23                      white -0.217883  0.374615   0.156732    2417\n",
       "8                heterosexual -0.205882  0.323529   0.117647     124\n",
       "10  homosexual_gay_or_lesbian -0.204301  0.365591   0.161290    1040\n",
       "17    other_race_or_ethnicity -0.196078  0.274510   0.078431      56\n",
       "15                     muslim -0.188039  0.288793   0.100754    2170\n",
       "5                    buddhist -0.181818  0.272727   0.090909      45\n",
       "22                transgender -0.173709  0.267606   0.093897     236\n",
       "3                    bisexual -0.172414  0.379310   0.206897      32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 groups with biggest FPR increase (fused - base):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>ΔFPR</th>\n",
       "      <th>base_FPR</th>\n",
       "      <th>fused_FPR</th>\n",
       "      <th>base_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>intellectual_or_learning_disability</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other_sexual_orientation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>other_gender</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL</td>\n",
       "      <td>-0.041768</td>\n",
       "      <td>0.065225</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>180488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>christian</td>\n",
       "      <td>-0.066788</td>\n",
       "      <td>0.100444</td>\n",
       "      <td>0.033655</td>\n",
       "      <td>4057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>other_religion</td>\n",
       "      <td>-0.081081</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asian</td>\n",
       "      <td>-0.090244</td>\n",
       "      <td>0.153659</td>\n",
       "      <td>0.063415</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female</td>\n",
       "      <td>-0.094791</td>\n",
       "      <td>0.151175</td>\n",
       "      <td>0.056384</td>\n",
       "      <td>5380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>male</td>\n",
       "      <td>-0.104010</td>\n",
       "      <td>0.176441</td>\n",
       "      <td>0.072431</td>\n",
       "      <td>4476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atheist</td>\n",
       "      <td>-0.134921</td>\n",
       "      <td>0.182540</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  group      ΔFPR  base_FPR  fused_FPR  base_n\n",
       "11  intellectual_or_learning_disability  0.000000  0.000000   0.000000      13\n",
       "19             other_sexual_orientation  0.000000  0.000000   0.000000       3\n",
       "16                         other_gender  0.000000  0.000000   0.000000       2\n",
       "0                                   ALL -0.041768  0.065225   0.023457  180488\n",
       "6                             christian -0.066788  0.100444   0.033655    4057\n",
       "18                       other_religion -0.081081  0.162162   0.081081      40\n",
       "1                                 asian -0.090244  0.153659   0.063415     450\n",
       "7                                female -0.094791  0.151175   0.056384    5380\n",
       "14                                 male -0.104010  0.176441   0.072431    4476\n",
       "2                               atheist -0.134921  0.182540   0.047619     133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ensures val_text is a pandas Series so it has original row indices\n",
    "assert hasattr(val_text, \"index\"), \"val_text must be a pandas Series\"\n",
    "\n",
    "val_idx = val_text.index\n",
    "\n",
    "# Pull identity metadata for validation set\n",
    "val_meta = train_df.loc[val_idx, identity_cols].copy()\n",
    "\n",
    "# get prediction scores for both models\n",
    "base_scores = clf.predict_proba(X_val)[:, 1]\n",
    "fused_scores = clf_fused.predict_proba(X_val_fused)[:, 1]\n",
    "\n",
    "# Choose thresholds:\n",
    "BASE_THR = 0.5\n",
    "FUSED_THR = 0.8179\n",
    "\n",
    "# Converts probabilities into hard predictions 0 or 1\n",
    "base_pred = (base_scores >= BASE_THR).astype(int)\n",
    "fused_pred = (fused_scores >= FUSED_THR).astype(int)\n",
    "\n",
    "y_val_arr = np.asarray(y_val, dtype=int)\n",
    "\n",
    "# metric helpers\n",
    "def rates_from_preds(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns rates + confusion counts.\n",
    "    FPR = FP / (FP + TN)\n",
    "    FNR = FN / (FN + TP)\n",
    "    \"\"\"\n",
    "    # creates confusion matrix and extracts:\n",
    "    # TN = correct non-toxic\n",
    "    # FP = non-toxic wrongly predicted toxic (false positives)\n",
    "    # FN = toxic missed (false negatives)\n",
    "    # TP = correct toxic\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "\n",
    "    # False Positive Rate: Out of all truly non-toxic comments, how many got wrongly flagged toxic?\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
    "    \n",
    "    # False Negative Rate: Out of all truly toxic comments, how many did the model miss?\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
    "\n",
    "    # True Positive Rate / Recall: Out of all toxic comments, how many did the model catch?\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else np.nan \n",
    "    \n",
    "    # Precision: When the model predicts toxic, how often is it correct?\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else np.nan \n",
    "\n",
    "    # prevalence (toxicity rate): What % of comments in that group are actually toxic?\n",
    "    tox_rate = y_true.mean() if len(y_true) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"n\": int(len(y_true)),\n",
    "        \"tox_rate\": float(tox_rate),\n",
    "        \"TN\": int(tn), \"FP\": int(fp), \"FN\": int(fn), \"TP\": int(tp),\n",
    "        \"FPR\": float(fpr), \"FNR\": float(fnr),\n",
    "        \"TPR\": float(tpr), \"PPV\": float(ppv)\n",
    "    }\n",
    "\n",
    "def subgroup_mask(identity_series: pd.Series, thr: float = 0.5):\n",
    "    \"\"\"Identity present if column >= thr.\"\"\"\n",
    "    return identity_series.fillna(0.0).values >= thr\n",
    "\n",
    "\n",
    "# build fairness table (per identity + overall)\n",
    "rows = []\n",
    "\n",
    "# Overall (all val)\n",
    "rows.append({\n",
    "    \"group\": \"ALL\",\n",
    "    \"base\": rates_from_preds(y_val_arr, base_pred),\n",
    "    \"fused\": rates_from_preds(y_val_arr, fused_pred),\n",
    "})\n",
    "\n",
    "# Identity groups\n",
    "for col in identity_cols:\n",
    "    m = subgroup_mask(val_meta[col], thr=0.5)\n",
    "    if m.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    rows.append({\n",
    "        \"group\": col,\n",
    "        \"base\": rates_from_preds(y_val_arr[m], base_pred[m]),\n",
    "        \"fused\": rates_from_preds(y_val_arr[m], fused_pred[m]),\n",
    "    })\n",
    "\n",
    "# Convert to a readable dataframe\n",
    "def flatten_row(r):\n",
    "    out = {\"group\": r[\"group\"]}\n",
    "    for prefix, d in [(\"base\", r[\"base\"]), (\"fused\", r[\"fused\"])]:\n",
    "        out[f\"{prefix}_n\"] = d[\"n\"]\n",
    "        out[f\"{prefix}_tox_rate\"] = d[\"tox_rate\"]\n",
    "        out[f\"{prefix}_FPR\"] = d[\"FPR\"]\n",
    "        out[f\"{prefix}_FNR\"] = d[\"FNR\"]\n",
    "        out[f\"{prefix}_TPR\"] = d[\"TPR\"]\n",
    "        out[f\"{prefix}_PPV\"] = d[\"PPV\"]\n",
    "        out[f\"{prefix}_FP\"] = d[\"FP\"]\n",
    "        out[f\"{prefix}_FN\"] = d[\"FN\"]\n",
    "    # deltas (fused - base)\n",
    "    out[\"ΔFPR\"] = out[\"fused_FPR\"] - out[\"base_FPR\"]\n",
    "    out[\"ΔFNR\"] = out[\"fused_FNR\"] - out[\"base_FNR\"]\n",
    "    out[\"ΔPPV\"] = out[\"fused_PPV\"] - out[\"base_PPV\"]\n",
    "    out[\"ΔTPR\"] = out[\"fused_TPR\"] - out[\"base_TPR\"]\n",
    "    return out\n",
    "\n",
    "fair_df = pd.DataFrame([flatten_row(r) for r in rows])\n",
    "\n",
    "# Sort to see where FPR improved/worsened most\n",
    "fair_df_sorted = fair_df.sort_values(by=\"ΔFPR\")\n",
    "\n",
    "# Pretty formatting for display\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "print(\"Fairness comparison (negative ΔFPR means fused reduces false positives):\")\n",
    "display(\n",
    "    fair_df_sorted[\n",
    "        [\"group\",\n",
    "         \"base_n\", \"base_tox_rate\", \"base_FPR\", \"base_FNR\", \"base_PPV\", \"base_TPR\",\n",
    "         \"fused_n\", \"fused_tox_rate\", \"fused_FPR\", \"fused_FNR\", \"fused_PPV\", \"fused_TPR\",\n",
    "         \"ΔFPR\", \"ΔFNR\", \"ΔPPV\", \"ΔTPR\",\n",
    "         \"base_FP\", \"fused_FP\", \"base_FN\", \"fused_FN\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# summary diagnostics\n",
    "print(\"\\nThresholds used:\")\n",
    "print(f\"  BASE_THR : {BASE_THR}\")\n",
    "print(f\"  FUSED_THR: {FUSED_THR}\")\n",
    "\n",
    "# Where fused reduces FPR the most\n",
    "top_improve = fair_df.sort_values(\"ΔFPR\").head(10)[[\"group\", \"ΔFPR\", \"base_FPR\", \"fused_FPR\", \"base_n\"]]\n",
    "top_worsen  = fair_df.sort_values(\"ΔFPR\", ascending=False).head(10)[[\"group\", \"ΔFPR\", \"base_FPR\", \"fused_FPR\", \"base_n\"]]\n",
    "\n",
    "print(\"\\nTop 10 groups with biggest FPR reduction (fused - base):\")\n",
    "display(top_improve)\n",
    "\n",
    "print(\"\\nTop 10 groups with biggest FPR increase (fused - base):\")\n",
    "display(top_worsen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3fe11",
   "metadata": {},
   "source": [
    "Fairness by Identity Group result:\n",
    "\n",
    "The fused TF–IDF + emotion model consistently reduces false positive rates across most identity groups compared to the TF–IDF-only baseline. This indicates that incorporating emotional context helps the classifier avoid incorrectly labeling identity-related but non-toxic comments as toxic. While this comes with a modest increase in false negatives for some groups, the overall effect suggests an improvement in fairness by reducing spurious toxicity flags.\n",
    "\n",
    "Example: black: base_FPR 0.409 → fused_FPR 0.184  (ΔFPR ≈ -0.225) -> after adding emotions there was large reductions in false positives, especially for sensitive identity groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a24e7",
   "metadata": {},
   "source": [
    "## 9. Testing custom comments\n",
    "\n",
    "I will now test some custom comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "add38895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds: baseline=0.5, fused=0.8179\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>top_emotions</th>\n",
       "      <th>baseline_prob</th>\n",
       "      <th>baseline_label</th>\n",
       "      <th>fused_prob</th>\n",
       "      <th>fused_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I totally disagree with you, but I respect you...</td>\n",
       "      <td>disapproval:0.838, approval:0.180, annoyance:0...</td>\n",
       "      <td>0.082825</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You're a complete idiot and nobody wants you h...</td>\n",
       "      <td>anger:0.529, annoyance:0.490, neutral:0.057, d...</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm really upset about how people like me are ...</td>\n",
       "      <td>disappointment:0.614, sadness:0.289, annoyance...</td>\n",
       "      <td>0.090693</td>\n",
       "      <td>0</td>\n",
       "      <td>0.094339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is awesome, thanks for sharing!</td>\n",
       "      <td>gratitude:0.974, admiration:0.604, approval:0....</td>\n",
       "      <td>0.010653</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I’m honestly sick of people like you pretendin...</td>\n",
       "      <td>annoyance:0.577, disappointment:0.199, anger:0...</td>\n",
       "      <td>0.617109</td>\n",
       "      <td>1</td>\n",
       "      <td>0.780222</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You contribute nothing of value, and everyone ...</td>\n",
       "      <td>annoyance:0.320, neutral:0.314, disapproval:0....</td>\n",
       "      <td>0.321746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.319636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Your presence here is exhausting, and I have n...</td>\n",
       "      <td>disappointment:0.434, annoyance:0.352, sadness...</td>\n",
       "      <td>0.294113</td>\n",
       "      <td>0</td>\n",
       "      <td>0.494503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>People like you make this place unbearable.</td>\n",
       "      <td>annoyance:0.301, sadness:0.137, disgust:0.105,...</td>\n",
       "      <td>0.326752</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503791</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  I totally disagree with you, but I respect you...   \n",
       "1  You're a complete idiot and nobody wants you h...   \n",
       "2  I'm really upset about how people like me are ...   \n",
       "3               This is awesome, thanks for sharing!   \n",
       "4  I’m honestly sick of people like you pretendin...   \n",
       "5  You contribute nothing of value, and everyone ...   \n",
       "6  Your presence here is exhausting, and I have n...   \n",
       "7        People like you make this place unbearable.   \n",
       "\n",
       "                                        top_emotions  baseline_prob  \\\n",
       "0  disapproval:0.838, approval:0.180, annoyance:0...       0.082825   \n",
       "1  anger:0.529, annoyance:0.490, neutral:0.057, d...       0.999989   \n",
       "2  disappointment:0.614, sadness:0.289, annoyance...       0.090693   \n",
       "3  gratitude:0.974, admiration:0.604, approval:0....       0.010653   \n",
       "4  annoyance:0.577, disappointment:0.199, anger:0...       0.617109   \n",
       "5  annoyance:0.320, neutral:0.314, disapproval:0....       0.321746   \n",
       "6  disappointment:0.434, annoyance:0.352, sadness...       0.294113   \n",
       "7  annoyance:0.301, sadness:0.137, disgust:0.105,...       0.326752   \n",
       "\n",
       "   baseline_label  fused_prob  fused_label  \n",
       "0               0    0.075663            0  \n",
       "1               1    0.999995            1  \n",
       "2               0    0.094339            0  \n",
       "3               0    0.012543            0  \n",
       "4               1    0.780222            0  \n",
       "5               0    0.319636            0  \n",
       "6               0    0.494503            0  \n",
       "7               0    0.503791            0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    emo_model\n",
    "    emo_tokenizer\n",
    "    emotion_labels\n",
    "except NameError:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "    GOEMOTIONS_MODEL = \"SamLowe/roberta-base-go_emotions\"\n",
    "    emo_tokenizer = AutoTokenizer.from_pretrained(GOEMOTIONS_MODEL)\n",
    "    emo_model = AutoModelForSequenceClassification.from_pretrained(GOEMOTIONS_MODEL)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    emo_model.to(device)\n",
    "    emo_model.eval()\n",
    "\n",
    "    emotion_labels = [\n",
    "        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "        'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "        'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "        'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "        'remorse', 'sadness', 'surprise', 'neutral'\n",
    "    ]\n",
    "\n",
    "# Make sure model is in eval mode and we know its device\n",
    "emo_model.eval()\n",
    "_device = next(emo_model.parameters()).device\n",
    "\n",
    "# -------------------------------\n",
    "# 2) GoEmotions inference (sigmoid for multi-label)\n",
    "# -------------------------------\n",
    "def goemotions_probs(texts, batch_size=32, max_len=128):\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = emo_tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            enc = {k: v.to(_device) for k, v in enc.items()}\n",
    "\n",
    "            logits = emo_model(**enc).logits\n",
    "            probs = torch.sigmoid(logits)  # IMPORTANT: multi-label emotions\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    return np.vstack(all_probs)\n",
    "\n",
    "def topk_emotions(probs_row, k=5):\n",
    "    idx = np.argsort(probs_row)[::-1][:k]\n",
    "    return [(emotion_labels[i], float(probs_row[i])) for i in idx]\n",
    "\n",
    "# -------------------------------\n",
    "# 3) Predict toxicity: baseline + fused\n",
    "# -------------------------------\n",
    "def predict_toxicity(texts):\n",
    "    texts = [str(t) for t in texts]\n",
    "\n",
    "    # Emotions\n",
    "    E = goemotions_probs(texts, batch_size=32, max_len=128)\n",
    "\n",
    "    # TF–IDF\n",
    "    X_txt = tfidf.transform(texts)\n",
    "\n",
    "    # Baseline\n",
    "    base_prob = clf.predict_proba(X_txt)[:, 1]\n",
    "    base_pred = (base_prob >= BASE_THR).astype(int)\n",
    "\n",
    "    # Fused\n",
    "    E_sp = sp.csr_matrix(E.astype(np.float32))\n",
    "    X_fused = sp.hstack([X_txt, E_sp], format=\"csr\")\n",
    "    fused_prob = clf_fused.predict_proba(X_fused)[:, 1]\n",
    "    fused_pred = (fused_prob >= FUSED_THR).astype(int)\n",
    "\n",
    "    # Build output table\n",
    "    rows = []\n",
    "    for i, txt in enumerate(texts):\n",
    "        tops = topk_emotions(E[i], k=5)\n",
    "        rows.append({\n",
    "            \"text\": txt,\n",
    "            \"top_emotions\": \", \".join([f\"{e}:{p:.3f}\" for e, p in tops]),\n",
    "            \"baseline_prob\": float(base_prob[i]),\n",
    "            \"baseline_label\": int(base_pred[i]),\n",
    "            \"fused_prob\": float(fused_prob[i]),\n",
    "            \"fused_label\": int(fused_pred[i]),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -------------------------------\n",
    "# 4) Put your comments here\n",
    "# -------------------------------\n",
    "my_comments = [\n",
    "    \"I totally disagree with you, but I respect your opinion.\",\n",
    "    \"You're a complete idiot and nobody wants you here.\",\n",
    "    \"I'm really upset about how people like me are treated.\",\n",
    "    \"This is awesome, thanks for sharing!\",\n",
    "    \"I’m honestly sick of people like you pretending to be reasonable.\", # No direct insult word, strong annoyance / disgust\n",
    "    \"You contribute nothing of value, and everyone knows it.\", # indirect, anger + annoyance + contempt-like emotions\n",
    "    \"Your presence here is exhausting, and I have no patience left.\", # Lexically neutral-ish, emotionally very negative, high annoyance + anger\n",
    "    \"People like you make this place unbearable.\" # Short, no swear words, strong emotional rejection\n",
    "]\n",
    "\n",
    "demo_df = predict_toxicity(my_comments)\n",
    "print(f\"Thresholds: baseline={BASE_THR}, fused={FUSED_THR}\")\n",
    "display(demo_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f576fb1",
   "metadata": {},
   "source": [
    "In manual testing, cases where the fused model classified a comment as toxic while the text-only model did not were extremely rare. Instead, emotional features primarily reduced false positives by preventing emotionally expressive but non-abusive comments from being labeled as toxic. This behavior aligns with the observed reductions in false positive rates across identity groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263949a8",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This project shows that emotional signals alone are insufficient for toxicity detection, but when combined with lexical features, they provide valuable contextual information. The fused model reduces false positives, particularly for identity-related comments, without sacrificing overall performance. These findings highlight the role of emotions in improving fairness and reliability rather than raw accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
